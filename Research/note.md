https://medium.com/@jonathan_hui/rl-natural-policy-gradient-actor-critic-using-kronecker-factored-trust-region-acktr-58f3798a4a93
trust region 优化: 是优化lower bound 函数的过程
自然梯度是从Trust region推导过来的
满足Trust region的策略梯度,推导出最终参数的梯度就是自然梯度



# 稀疏解
对稀疏解的一个直观理解是，在使得Loss最小的情况下， 尽可能的让更多的$W$的L1 Norm接近0，即获得一个稀疏的$W$。
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/05/23/1590168850361-1590168850384.png)

# 点云法线
对邻居点做PCA之后， 特征值最小对应的那个基(在U中的最后一列)对应着原始点集方差最小的方向，也就是法线平面的反向，所有点基本都靠在这个平面上了。

# CMAKE pybind
CMAKE中 如果想要使用指定版本的python对应的pybind11，需要将对应版本python的inclue和lib都引入进来，如下例子所示，其中PYTHON_ROOT是python对应的environment的根目录，如~/anaconda/env/py36，PYTHON_VERSION是python的版本,如3.6
```
# python and pybind11
set(PYTHON_INCLUDE_DIRS "${PYTHON_ROOT}/include/;${PYTHON_ROOT}/include/python${PYTHON_VERSION}m")
set(PYTHON_LIBARIES "${PYTHON_ROOT}/lib/")

message("PYTHON INCLUDE DIRS: " ${PYTHON_INCLUDE_DIRS})
message("PYTHON LIBRARY: " ${PYTHON_LIBARIES})
include_directories(${PYTHON_INCLUDE_DIRS})

## compile mls upsample library
add_library(upsample_ext SHARED upsample.cpp upsample.hpp)
set_target_properties(upsample_ext PROPERTIES PREFIX "")
#target_link_libraries(upsample_ext pybind11::module ${PCL_LIBRARIES})
target_link_libraries(upsample_ext ${PYTHON_LIBARIES} ${PCL_LIBRARIES})
```

# 分组卷积
原始卷积： HxWxC1的fm，经过C2个KxKxC1的卷积核，得到HxWxC2的fm
分组卷积： 将输入按照C1划分为g组，经过C2个KxKx(C1/g)的卷积核， 同样得到得到HxWxC2的fm，注意这里，对于输入，每一组有(C1/g)个通道，而每一组输入对应着(C2/g)个卷积核，每一组输出得到的是HxWx(C2/g)的fm；

这样，卷积参数从C2xC1XKxK减小到了C2x(C/g)xKxK，减小到原来的g倍。	

而对于深度可分离卷积，其实就是：
1. 先用C1=C2=g的分组卷积，称之为深度卷积；
2. 再用1x1卷积，称之为逐点卷积；

# 一个粗暴的理解
- 公式表征方法：列方程；　
- 优化算法：解方程

# 梯度下降算法
梯度方向，就是函数值下降最快的方向，朝着梯度方向走就会使cost函数值下降。

引入二阶梯度的目的是，不仅直到函数值在某个方向是是不是上升/下降的，而且知道上升/下降的变化趋势，并可以适应性的调整梯度大小。例如在接近谷底的地方，根据二阶导数知道，梯度越来越缓，此时引入二阶梯度让参数更新更小的值，可以防止越过最优点；而在半山上，通过二阶梯度知道梯度还很大，此时引入二阶梯度更新更大的值，可以更快的到达谷底。

# 用PCA求出来的基底变换矩阵，可以任意调整三个底的顺序和数值，然后左乘到原始点云上即可得到新底下的点集； 即可以自己写一个新的基底矩阵(从I开始)进行变换

# 右手法则
123指头分别对应123轴，即
轴-手指-颜色
- X-大拇指-R
- Y-食指-G
- Z-中指-B

# PCL出错状态机:
在特定版本的eigen上装了pcl之后，再安装其他版本的eigen，大概率pcl中的很多module都不可用了

# 刚体变换的表现形式，与DL中参数和输入的线性组合方式非常像, R=W, t=b

# 将传统方法和深度学习融合, 其实就是传统方法中可学习/需要优化的参数连到NN中, 和普通的NN参数一起优化，即可以看成一个更大的更宏观的优化方法，求解最优解问题。在此过程中，只需要保证在梯度传播路径中，传统方法接入点的传播路径与NN的前后传播路径是导通的即可。

# 图优化建图方式
节点：优化变量
边：误差项

例子：节点1：相机位姿；节点2～N：特征点3D坐标；边1~N-1:从3D坐标估计值投影到相机位姿的成像平面后，与实际测量值的loss
BA就是同时调整相机位姿和观测得到的特征点3D坐标

pointcloud 描述子

用objectdetection的思路做global localization

# LiDAR Super Resolution
改进点：loss，网络，序列信息
效果较好：TCN，比原来要高8.几%

# 怎么鼓吹自己
1. 介绍方向重要
2. 自己做的方向很难
3. 自己做了什么，只说思路，不要说流程
4. 要比同行做得好，引用多少多少
5. 有什么成果

# 关于目标检测的scale
在2D图像中，因为投影的问题，scale很重要；
而在3D点云中，scale就没那么重要，因为同样的物体在不同距离大小是相同的。但是有另一个问题需要考虑，就是稀疏程度不同。

# 点云有光流吗？
光流描述的是图像中的像素运动，是通过光照产生的灰度不变假设计算出来的；对于点云来说，这样的假设在什么量上成立？

# KF
1. 卡尔曼滤波的预测过程，实际上就是用运动方程，通过上一个时刻的状态来预测当前时刻的状态，(比如已知上一时刻的位姿,已知运动为匀速运动，那么就可以预测下一时刻的位姿);
2. 更新过程，其实就是用bayes公式，以及观测方程(比如在某一个位姿上对路标点进行观测)，来计算状态的后验概率分布(即当前时刻的最终状态);
3. 对于EKF，是将运动方程和观测方程进行线性化(即一阶Taylor展开)，形式上就是将运动和观测方程的线性做乘换成函数形式，然后在运动方程构成的先验分布协方差上加一个一阶导的二次型。然后再按照1,2的步骤进行预测和更新。

# 边缘化
对于多变量的联合概率分布，例如X和Y，求单个变量的概率分布(称为边缘分布)，比如P(X)或者P(Y)的过程，叫做边缘化，这个过程中其实就是将其他不相关变量的概率求和(积分),其他变量就叫做被边缘化的量。比如求P(X)，那么就是将P(X|Y)对Y求积分。
在slam后端优化时，边缘化是指，因为在对H阵进行Schur消元后，是先单独求解出所有的相机位姿Xc，再求解路标Xp，因此从概率的角度来讲，就是先确定Xc的边缘分布，而将Xp进行边缘化。

在边缘化掉某个位姿变量之后,对新的H阵引入的稠密性,通过可视化可以看出来,其代表了剩下的位姿和路标点之间建立的新的联系,即边缘化位姿之后保留了剩下的路标和位姿之间的约束关系;将某个变量margin掉之后,就可以不用管它,然后先求解其他变量

# 在位姿图中，已知量只有每两个相机之间的相对位姿(用特征匹配或者其他方法)，未知量是每个相机的绝对位姿。

有一个关键问题：没有回环的时候怎么进行图优化？这个时候所有帧的相对位姿仅仅构成一个链，而不是图。

第一种思路:这时候可以考虑用这样的方法：除了在每帧之前用特征匹配计算相对位姿之外，还要对相互间隔不远的两个关键帧计算相对位姿，这样就会构成有约束的图了，图的边就有两种，一种是相邻帧的相对位姿，另一种是间隔了几帧的关键帧相对位姿，因为这两种位姿是独立计算出来的，因此构成图之后就会有误差，就有优化的空间。

第二种思路:
图的节点是关键帧的绝对位姿，其初始值是仅仅经过每一帧VO叠加之后得到的绝对位姿；
图的边(测量值)是两个关键帧之间经过局部优化(例如BA)之后得到的更准确的相对位姿；
这样图的节点之间和观测值就产生了误差，有优化的空间。

这么看起来，有路标点和位姿的BA在没有回环的时候就更像图优化；而没有回环的位姿图其实不构成图。

# g2o里,VertexSE3EXPMAP跟Edge传递的增量是6D的李代数向量；但是在内部更新变量和实际内部存储的时候，都是用的一个3D trans+4元数来做的。（这样设计可能是考虑优化时李代数直观，实现时四元数方便？）

# g2o里，在对Vertex的优化变量进行增量更新时，要注意 增量 和 估计值的乘法是不满足交换律的，一般要左乘增量；

# 坐标系A到坐标系B的正交转换
尝试从A旋转到B,在旋转的过程中看每一步绕哪个轴转了多少度,最后就可以用transforms3D中的eular2mat计算出从系A到系B的旋转矩阵R,这个矩阵左乘原向量就得到新坐标系下的向量.

# UTM坐标系就是我们通常所说的平面直角坐标系

# Cartographer要在install_isolated的launch和config里边更改配置文件,如果想离线建图,要使用offline node

# PYBIND11到底怎么玩
如果没有find_package(PCL) 完全按照pybind11的文档

如果find_package(PCL) 必须加上
include_directories(/home/hyx/miniconda3/envs/torch/include/python3.6m)
其中include_directories中的参数是你希望编译到的目标python版本的include目录
可以通过
python -c "from sysconfig import get_paths;print(get_paths()['include'])"
查到


# transformer in CV
transformer最大的优势是可以并行处理一个序列（用attention），而不像RNN之类的必须要先处理一个词再接着下一个

transformer在分类上结果，小数据集没有cnn好，但是大数据集预训练之后效果比部分cnn好得多

可能的原因：
1. CNN是参数共享，transformer里有很多fc的，参数量多一些，数据量更大可能会更好一点；
2. 全attention结构确实比CNN能够更好的建模全局信息；

# 双目也并不都是用来估计稠密深度的， 可以在求出来匹配的特征点之后三角化出绝对的深度

# 3d slam提特征与上采样的关系
目前来看，提特征跟点云密度关系其实不大，如果想要对点云进行上采样的话，重点是把对应有效特征的那部分点上采样出来


# GAN生成图像质量评价标准

## 1. Inception Score
将生成图像送进inception, 得到1x1000的预测向量; 

	1. 如果生成的样本比较真实,那么Inception应该给出某个比较明确的类别, 向量中某个值比较高, 因此预测向量的信息熵越低越好;

	2. 如果生成的样本多样性好,那么对于产生的一批样本,其对应的这批类别标签的分布应该比较均匀(各个类别都生成了一些), 因此类别标签的信息熵越高越好;

	3. 最后用样本和标签的互信息来计算Inception Score, 互信息的计算方法就是求2-1的差值; 等价于标签分布和标签-样本条件分布的KL散度;

	4. 缺陷: 

		1. GAN过拟合时,如果生成的完全都是训练集的真实图像一样的图像, 那么Inception score也会很低; 

		2. 偏爱ImageNet中的物体类别，而不是注重真实性。GAN生成的图片无论如何逼真，只要它的类别不存在于ImageNet中，IS也会比较低;

		3. 若GAN生成类别的多样性足够，但是类内发生模式崩溃问题，IS无法探测;

		4. IS只考虑生成器的分布, 而忽略数据集的分布;

		5. IS的高低会受到图像像素的影响


## 2. FID, Frechet Inception Distance score

	1. 分别把生成器生成的样本和判别器生成的样本送到分类器中, 抽取分类器的中间层的抽象特征, 假设该抽象特征符合多元高斯分布;

	2. 估计生成样本和真实样本产生的特征的均值和方差, 这样就拿到了生成和真实这两个分布的参数;

	3. 计算这两个分布的Fréchet距离, 生成图像质量的度量;
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2021/01/07/1610014718100-1610014718101.png)


## 3. MMD, Maximum Mean Discrepancy

	1. 找一个核函数k将两个样本映射为一个实数,这个核函数需要能度量两个样本的相似程度,越相似函数值越高;

	2. 用所有的样本来计算MMD距离, 其可以来度量生成分布和真实数据集分布的相似程度;

![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2021/01/07/1610014680718-1610014680719.png)


## 4. Wasserstein Distance

	1. 首先要有一个训练好的判别器D;

	2. 对生成图像和训练集图像, 分别用判别器来计算出判别值;

	3. 然后用所有的判别值来计算Wasserstein距离;

	4. 这个评价指标可以探测到生成样本的简单记忆情况和模式崩溃情况, 但是因为D是在特定数据集上训练出来的, 只能用来评价这个数据集上的图像, 比如在苹果图像上训练的,就不能用来判别橘子并算Wasserstein距离;


## 5. one-Nearest Neighbor Classifier

	1. 把生成的图像打上0标签, 真实图像打上1标签, 混合在一起构成数据集D(共2n张图像);

	2. 每次从D中拿出2n-1个样本,训练一个1-NN(k=1的KNN)分类器;

	3. 用剩下的1个样本做测试; 如此重复留一法验证2n次,直到所有样本都被验证一遍,计算最终的准确率;

	4. 如果准确率接近0%, 说明出现简单记忆现象: 每个生成样本都和真实样本距离很近, 在1-NN查找的时候, 永远只能找到和留一的生成(真实)样本对应那个真实(生成)样本; 
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2021/01/07/1610014530089-1610014530089.png)
	
	5. 如果准确率接近100%, 说明生成的样本分布和真实样本分布差的很远,很容易就能够区分开,说明生成的质量很差; 
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2021/01/07/1610014539557-1610014539558.png)

	6. 当准确率接近50%的时候, 才说明生成和真实样本的距离接近, 而又不是过拟合导致很近;  
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2021/01/07/1610014435334-1610014435335.png) 
	

## 6. NRDS, Normalized Relative Discriminative Score

可用于多个GAN模型的比较, 想法是：实践中，对于训练数据集和GAN生成器生成的样本集，只要使用足够多的epoch，总可以训练得到一个分类器C，可以将两类样本完全分开，使得对训练数据集的样本，分类器输出趋于1，对GAN生成的样本，分类器输出趋于0。

	1. 在每个epoch中, 对n个GAN生成的样本打上标签0, 对真实样本打上标签1, 合成一批训练集;

	2. 用合成的数据集训练模型一个epoch; 

	3. 然后当前训练的模型对n个GAN的数据进行测试, 记录输出(0~1的值, 越接近1说明分类器认为这个图像越像真实样本)和当前的epoch;

	4. 这样经过多个epoch, 对于每个GAN的数据, 都能绘制一条 epoch-输出 曲线, 这条曲线面积越大, 说明在分类器用了越少的epoch来认为某个GAN的样本是真实图像, 这就能说明生成图像的质量更高;  

![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2021/01/07/1610014399416-1610014399443.png)
	
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2021/01/07/1610014370518-1610014370620.png)


## 7. GANtrain-GANtest

是一种评价流程, 定义真实图像训练样本集St, 真实图像验证集Sv, 生成图像样本集合Sg; 

	1. 在真实图像St上训练分类器, 并在真实验证集Sv上测试, 准确率记为GANBase;

	2. 在生成图像上Sg训练分类器, 并在真实验证集Sv上测试, 准确率记为GANtrain;

	3. 在真实图像St上训练分类器, 并在生成图像Sg上测试, 准确率记为GANtest;

	4. 比较GANBase和GANtrain, train较小, 说明GAN存在问题, 可能是发生了模式丢失, 或者样本不够真实, 让分类器学不到有效的特征; 或者生成的类别分的不够开; 如果二者比较接近, 说明生成图像的质量较高, 和训练集有很多模式相似性;

	5. 比较GANBase和GANtest, 如果test较高, 说明发生了过拟合, 发生了简单记忆问题; 如果test较低, 说明生成的数据分布不好, 图像质量不高;  


## 8. SSIM, PSNR, SD

这一系列是对图像质量进行评估, 主要是来评估图像的锐度/对比度/亮度分布, 而不是基于图像内容进行评价; 
适合任务:超分辨, 压缩后重建等 GT和预测值 以及 输入和输出值 有较明确的映射关系的任务

- SSIM, 结构相似度, 基本思路是基于图像局部灰度的均值方差,来计算亮度差异,对比度差异,以及结构差异,算法参考WIKI;

- PSNR, 峰值信噪比, 基本思路是求两幅图像逐像素差的倒数, 算法参考WIKI;

- SD, 锐度差异, 算法类似PSNR, 但是比较的是两幅图像的锐度差;


## 9. Perceptual loss

类似vid2vid中的feature matching loss, 用某种Encoder, 对真实图像和生成图像分别进行encode, 然后对输出的feature进行比较;


## 10. 借助分类/检测/分割任务进行评估

在生成图像上进行分类/检测/分割任务, 本质上和Perceptual loss, 以及GANtrain-GANtest中的Base-test比较类似,


## 11. Forward / Backward Consistency

这个是WC vid2vid针对他的模型提出的, 来判断视频生成任务中, 生成的视频是否有时间上的一致性: 

	1. 对于一个时长为t的视频序列的input, 将其顺序-逆序拼接在一起, 构成时长为2t的video input;

	2. 将video input作为输入, 送进模型中, 得到预测的视频输出;

	3. 比较第0帧和最后一帧的图像的差异(论文中用的就是逐像素差); 如果模型有时间一致性, 那么第0帧和最后一帧应该是完全一致的(同理第1和第n-1, 第2和第n-2等也应该是一致的);


## 12. subjective scores

人类评估, 目前已知的工具有Amazon Mechanical Turk (AMT), 在测试中, 随机 同时 给两个video(图片), 不告诉被测试者是什么模型生成的, 从几个维度上评价, 选择出哪个结果更真实;


## 13. Synthetic-Neuroscore

参考论文Synthetic-Neuroscore: Using a neuro-AI interface for evaluating generative adversarial networks

让人类在看GAN生成图像和真实图像的时候, 分别带上EEG设备采集脑电信号, 然后通过对EEG进行分类和对比, 来确定生成的图像是否和真实图像一致;



# torch里的DataParallel是把batchsize平均分到多个gpu上，这里要看一下跨卡batch norm

# 局部坐标系的点云转换到世界坐标系
对于局部坐标系(LiDAR为中心的点云)Xl, 现在有其在世界坐标系下，以起点为原点，当前位置为终点的的里程计P(w->l),或者称之为当前位姿，那么这一帧点云在世界坐标系下的坐标Xw就是

Xw = P(w->l) Xl



# squeezenet训练

注意图像分辨率小的时候，不要进行过多的降采样，否则会使信息丢失

ATLAS的AIPP在读取图片的时候，要注意RBG通道的顺序要和网络训练的顺序一致，并且图像的归一化/缩放参数也要和网络训练的一致。



# 坐标系转化原则

从A坐标系的姿态转换到B坐标系的姿态，要用B->A的转换，左乘A坐标系下的姿态；

其中B->A的转换是一个相对位姿，其绝对的值就是都以B系为参考，以B的原点为起点，看A系的原点在B系下是什么位姿，那么B->A就是什么变换；

注意这里说的A坐标系的姿态，就是某个姿态在以A为原点的系下的位置(姿态)

个人总结用以下表示方式来表示：

- 某个物体x(或位姿轴)在A系下的位姿：$P_{x_A}$,或者直接表示成$P_{A}$
- B->A的转换：用$P_{B->A}$来表示，其等价于在B系下的A原点的位姿，即$P_{A_B}$，有
	$P_{x_B} = P_{B->A} P_{x_A}$
或写作
	$P_{x_B} = P_{A_B} P_{x_A}$ 
或写作
	$P_B = P_{B->A} P_A$

×××××× 欧拉角在进行旋转时，所绕的三个轴都是以原始位姿作为参考来转的，而不是转了一个轴之后，以新的轴为参考再转下一个轴！！！！

右手规则参考 https://gtsam.org/2021/02/23/uncertainties-part2.html

xxxx 首端变换坐标系要左乘, 末端变换坐标系要右乘

xxxx
对于旋转量正好是pi的倍数的情况, 可以直接通过观察SO3的行向量(左乘的情况), 比如  
    [0 -1 0]
R = [-1 0 0]
    [0  0 1]
这种情况, 第一行是[0,-1, 0], 就是将源坐标的y值反转, 即原来的y轴被旋转了180; 第二行就是将源坐标的x反转
这样方式可以在旋转量为整数的情况下,快速直观计算旋转矩阵

xxx 对于VINS中的坐标表示方法
imu^T_cam 表示 在imu系下观测得到的cam位姿

也有说法是, A到B的变换矩阵, 是经过这矩阵左乘之后, 把坐标系A下的点带到坐标系B下;

已知两个时刻的W系下的全局定位$B_{k}$和$B_{k-1}$, 求两个定位之间的里程计, 一定要注意严格使用坐标转换原则

$$T_{W->B_{k-1}} = T_{W->B_{k}} T_{B_{k}->B_{k-1}} $$

所以里程计是

$$T_{B_{k}->B_{k-1}} =  T_{W->B_{k}}^{-1} T_{W->B_{k-1}} $$

$$T_{B_{k-1}->B_{k}} =  (T_{W->B_{k}}^{-1} T_{W->B_{k-1}})^{-1} $$

这里把每个时刻的位姿也看做不同传感器之间的位姿, 这样可以保证一致性

# temperature parameter
任意公式中的尺度缩放参数


# interactive slam

在地图有整体旋转(传感器安装导致的)时,不要强行加ground约束, 而是应该
1. 先把所有回环加上
2. 加xy方向的直线约束



# 20210410
1. LIOSAM,IMU+轮速计
2. 多数据源融合,置信度选择问题
3. 相机-雷达外参标定, 多标定板+标定板相互之间的位置约束
4. IMU-LiDAR的transition不需要精确标定,是因为imu估计的A已经有很大的误差了, 积分之后得到的位姿误差会远大于标定不准导致的误差
5. 里程计-相机;里程计imu标定
6. 尝试eskf进行位姿估计

# 阿卡曼车辆的定位问题
当且仅当定位源与车辆后轮中心(实际运动的转向中心)重合时, 里程计轨迹的切线方向才会与航向角重合

# informer作者经验分享
1. 刷会议，slidelive
2. wiki how
3. abstract 问题要先抛出来；最后一句要总结贡献
4. contribution要用项目编号，明确
5. rebuttal，要先提出reviewer问了什么问题
6. cooperation
7. 交流合作讨论
对scoop怎么看，您这个工作很solid，做了很长时间，如果这段时间scoop了，你觉得应该怎么处理，或者您有被scoop的经历可以分享

1. 一般来说scoop不会完全一致，如果被人scoop了，说明方向是可行的，并且可以避开一些坑
2. 所以尽量要做热点问题，这样scoop完全一致的概率就很小
3. 如果是做old topic，被经典问题scoop，就要考虑是不是要继续做了
4. 高年级要做热点，这样scoop的几率会小很多；低年级可以尝试做深入的问题

# 对于I-XXX系统， IMU的ba/bg的优化值可以用来检测系统是否估计的准确（错误的估计优化出来的ba/bg可能非常高），参考LVI-SAM

# Fixed-Lag Smoothing
也就是滑动窗口优化（Sliding Window Optimization）．状态向量包含随时间滑动的窗口内多个状态．但是也需要将旧状态边缘化到高斯先验中．因此在滤波算法中存在的边缘化问题，这里都存在．
但是由于采用了多个状态的窗口，状态估计更精确，同时可以建立鲁棒的优化代价函数，降低外点对状态估计的影响．
状态向量中可以加入测量的structure，但是太多strcuture会造成高斯先验矩阵是稠密的，通常会想办法减少structure的数量．

# IMU预积分的目的

参考这个 
视觉惯性里程计的IMU预积分模型 - LZ紫色大智的文章 - 知乎 https://zhuanlan.zhihu.com/p/90213963

因为IMU积分的当前pvq是通过
上一个时刻的pvq + 当前时间段的积分(积分过程依赖这个时间段中每个时刻的q)
计算得到的，每次当优化更新了上一时刻的pvq之后，如果想获得当前准确的pvq，就必须重新计算整个积分过程(因为当前时间段的积分在每个时刻都依赖最新的q)，这可能非常耗时。
使用了预积分之后，只需要在第一次计算预积分项，后边对历史状态的pvq更新的时候，不需要重新计算预积分项，只要把i时刻的q更新一下重新算一遍即可。

# gtsam的between factor，添加的量是两个factor V(t-1),V(t)之间的相对移动，即以V(t-1)为原点的坐标系下观察到的V(t)的坐标


# 如果系统是线性的(AX+b形式)，就可以写出闭式解；如果是非线性，就很难写出闭式解。
![20210511215450](https://cdn.jsdelivr.net/gh/HViktorTsoi/gitnote-image@master/PicGo/20210511215450.png)

但是，对于非线性问题，将其在某个点处线性化之后，就又可以用闭式解的方式求解了，但是这种闭式解的得到的只可以是一个小的增量，而不能像线性问题一样的全局解，因为这个解只有在线性点附近才是比较准确的。


这样的话，通过不断的线性化-》更新增量-》线性化-》更新增量。。。。就可以逐渐收敛到最优解附近

# 3D旋转矩阵有9个参数，但是实际上只表征了3个方向的信息。由于矩阵的正交特征等约束，使得这个矩阵的inherent dimensionality仍然是3，实际上也是这个9D流型切空间的维度。

# SE3的伴随性质 https://gtsam.org/2021/02/23/uncertainties-part3.html
伴随性质可以用来将左乘转换成右乘，从而便于来求位姿的inverse、composition以及relative transformation的方差分布。

# BLH2XYZ中，XYZ是以地球质心为原点，地心-北极为Z轴，赤道平面为xOy平面的3D直角坐标系，因此直接用BLH2XYZ转换出来的坐标是不能用的，因为当车在地面行走时，这个轨迹相当于是相对于地球球心，贴在地面的一个空中的倾斜轨迹；如果想直接用于驾驶，必须转为相对于地面平行的空间直角坐标系（比如UTM之类的，或者将BLH2XYZ的结果）

# 在ros robotlocalization的navsat_transform中，utm-odom的tf只用第一帧来计算


# while we can track the relationship between features which are far, far apart, we usually shouldn’t, since the theoretical gain in accuracy is tiny (or, in the case of lin- earized approaches, even negative).

# 在RS雷达中,如果没接时间同步线只连了pc, 那么是按照 接收到帧的系统时间 - 这一帧雷达旋转的时间 来计算消息时间戳的

# IMU内参标定
对于razor imu来说, 重要标定三类误差:
1. 用razor imu内置的板载程序来标定刻度误差, 主要包括传感器测量的无尺度三轴Acc的最大最小值; 用imu内置的程序来标定gyro的bias,标acc需要用六面法,不能有过激运动;标gyro需要静置10s
2. 用imu_tk来标定misalignment(消除三轴的非正交性), scale(三轴的测量缩放尺度, 否则三轴加速度合量就不等于g了), bias(三轴的偏置), 注意, 修正的测量 = mis * scale * (原始量 - bias), 需要给定g的初始值, 静置50s, 然后放置50个不同的姿态每个姿态1s
3. 使用imu_utils来标定噪声误差(liosam中的noise)和随机游走误差(liosam中的bias), 需要静置2小时

# pointnet++相比于pointnet, 并没有更好的解决位置敏感的问题, 因为kdtree每个节点的local区域, 提特征的时候仍然是位置敏感的, 但是其改进是, local区域一般都是更小的part, 这些part一般来说相对于整个场景更具有普遍意义, 因此用这种local的方法可能有更好的效果

# 一个比较反直觉的事实
1. 手持雷达建图时, 位置变化造成的运动畸变几乎可以忽略不计, 因为手臂的长度通常几十厘米, 即使很大的动作, 线速度也不会很大(加速度会很大, 但是这个不会导致雷达的运动畸变, 最多是点的噪声增加), 但是人体动作会产生很大的角速度, 想想一下你在1s之内 把手机从正面转到背面, 那角速度就达到180度/s了; 实际测量的时候, 用手快速转雷达, 经常会产生高达10 rad/s(大概500度/s的角速度, 这时候, 如果imu和lidar有0.01s的时间差, 那么雷达在这个期间已经跑出去5度了, 这时候50米距离会产生4米左右的误差, 此时imu-liar同步也比较重要)
2. 相反, 对于车载场景, 载体带着雷达的线速度非常大, 因此点云实际上会有很大的位置运动畸变, 而车辆前行时基本不会有角度的运动畸变, 但是车辆如果急转向, 就会有比较大的角速度

# 知识蒸馏
1. 首先有多个训练好的大模型
2. 给大模型和小模型输入同样的数据(这里可以使很多数据,不一定是训练集数据)
3. 用大模型的logits / 各层feature来监督小模型, 最终把大模型的知识蒸馏到小模型中
4. softmax的大于要求的类别, 这个对分类器影响不大

# HTTP后边有两个\r\n

# 问题: 如果雷达在做高速的匀速直线运动, 这样情况下加上slam, 运动畸变有影响吗?
有影响, 首先同一帧点云的起始到终止点就有很大影响

#
#
#
***************
1. 检查rs to velodyne的第一个point的时间戳与点云帧的timestamp是否一致!!!!!!!!!!!!!!!!!!
2. 消除运动畸变的时候, 要考虑每个点与位姿的对齐
3. !!!!!!!!!!!!!!! 对雷神做运动畸变消除的时候, 末点的时间不一定等于点云的最晚时间!!!!!!!!!!!!!!	
4. findRotationAndPosition 确定ins的数量能够覆盖整帧点云!!!!!
5. 雷达-惯导的z方向外参影响高度定位


# 固态雷达边缘提取
bleeding effect邻域 + 车道线 + 几何边缘 

所有平面上的反射率边缘（所有平面包括路边指示牌）

# INSIGHT
所有经典cv方法，是不是都可以改成端到端学习的方法？ 改了之后，是不是随着数据的增长效果都会变好？


# 反对称矩阵只有三个自由度, 所以可以和3维向量一一对应 *** 矩阵和向量建立联系

# VINS mono
1. 在载体静止, 且特征点都在远处, 近处没有足够约束, 但是有轻微的高频旋转晃动时, 轨迹会出现偏移, 并且此时尺度也会出现漂移
2. 地面有反光的情况, 因为反光的特征点是跟随相机一起移动的, 这种情况下VO倾向于认为自己有移动


# VINS fusion在跑的时候, 
配置文件中外参是IMU系下观测Camera的结果
1. 加上外参误差估计效果会更好;不加外参估计, 当imu和相机外参不准的时候, 会有严重的尺度偏移现象, 表现为整个轨迹比实际尺度大(比如实际尺度1m, vins估计出来就有1.1m)
2. 在已经做了时间同步的系统中, 加上时间误差估计会让效果变差
3. VINS对外参极其敏感, 外参的T或者R有任何大的误差, 在关闭在线外参标定后, 在初始化静止不动的时候, 整个位姿会抖动, 在载体运动起来之后, 整个轨迹和速度会漂移的很严重
4. 对于D435i, 确实只有用初始参数, 在关闭在线外参标定的时候能稳定的跑下来, 说明出厂参数是一个较好的外参估计(至少对于baseline来说是这样)
5. VINS Fusion应该是用imu到两个相机的外参来算baseline, 然后恢复深度的, 因此真实尺度依赖于baseline, 也就依赖于两个外参, 如果外参标的不准, 就会严重影响尺度, 并且造成偏移, 所以一般用出厂外参作为初始解, 并且最好把外参优化打开. 而VINS Mono虽然对外参也依赖, 但是没有VINS Fusion要求这么准确(考虑是不是要把baseline作为单独的参数给fusion用)
6. 对于T265, 左右双目的KB内参和畸变参数都不同, 如果误用同一套参数,会导致最终的3D特征点有很大的误差,这里一定要注意

*** VINS 系列一个非常重要的insight: imu的参数要仔细调整, 如果使用了差的imu, 要把acc_n等参数都调大, 这样在feature少的地方才不会造成漂移, 同时系统的鲁棒性会上升(但精度可能会受影响)

**** VINS fusion跑双目鱼眼的一个巨坑: 一定要用fisheye mask把鱼眼最外圈的feature屏蔽掉, 否则鱼眼边缘的异常特征点会严重影响tracking, 从而导致双目slam发散, 这也是最开始fusion跑T265和自组双目失败的原因(而pinhole相机就没有这个问题); 另外, MEI模型要比KB模型对鱼眼边缘的feature处理更加稳定, 因此如果要使用KB模型的话, mask要屏蔽掉更多的鱼眼边缘点(但这会导致可用feature变少); 一个更好的mask也可以减小outlier的影响(如相机遮挡物等), 使结果更加鲁棒

7. 目前观测到的一个现象: vins fusion跑的效果, 跟imu的方差有关, imu方差设置的较小, 轨迹更精确, 但是更容易在feature less的位置崩溃; imu方差设置的大, 崩溃的概率更小, 但是更容易崩溃

8. VINS fusion imu对于低速稳定运动的场景适配并不好, 很多时候车还在匀速向前跑, 但是轨迹突然发散并崩溃; 或者车静止, 但是估计出来的位姿有偏移. 将imu去掉换成纯双目里程计之后, 轨迹不会发散, 鲁棒性增强, 但是整体轨迹累计误差会比有imu的情况大很多.

# 用RealSense D435i跑双目ORB slam的时候, 双目的R矩阵可以用出厂的参数, 在realsense viewer中看calibration data, 其中wrold to left和world to right分别是左右的R矩阵(待确认, 输出的图像是不是已经做了极线对准了)

# Realsense用rs-enumerate device查询外参的时候, Extrinsic from "A"	  To	  "B" 表示的是B->A的外参 

# Realsense T265支持融合轮式里程计, 但是输入的格式是轮速, 即底盘x y z三个方向的速度

# ORBSLAM 单目时 KF的mvDepth和MapPointMatches数量一致, 因此不会出现segmentation fault的情况; 但是双目的情况mvDepth只有MapPointMatches的一半左右, 但是在KeyFrameCulling的时候还是按照MapPointMatches的尺寸遍历mvDepth, 这就会产生越界的segmentation fault; 对于mvuRight同样有这个问题

改进方式: 在Frame.cc中的Frame::ComputeStereoFishEyeMatches函数里, 将mvDepth和mvuRight的初始化改为
```C++
mvDepth = vector<float>(N,-1.0f);
mvuRight = vector<float>(N,-1);
```
即初始化size改为N

# ORBSLAM在手持的时候如果不好初始化, 就用手持画立方体, 基本就可以提供足够多的加速度用来初始化

# MSCKF的外参定义
https://github.com/KumarRobotics/msckf_vio/issues/7

# Basalt的奇怪bug: 在录制rosbag的时候, 将之前录制的bag转换为euroc格式的bag, 从0.01秒开始转录, basalt就可以跑成功; 而从0秒开始转录, basalt在优化时就会出现很大的error, 然后se3出现nan崩溃. 但用用basalt的ros wrapper跑相同的原始bag的时候就没有问题; 这里猜测是录制rosbag的初始时间恰好imu或者图像的帧存在误差 

经过调试发现, garage对应的bag, 是先发出的第一帧图像topic, 然后再发出imu topic; 而buaa二楼的bag, 以及从0.01秒之后开始的garage.bag, 都是先发出的imu后发出的图像; 因此basalt在这里可能是有个初始化的假设, 即先要收到imu的预积分, 再收到图像进行优化才可以, 否则优化就会出现很大的误差(这里可能是同步topic的时候的时间处理问题, 需要检查代码)

# 视觉里程计和无人机避障的猜想
用里程计来提供局部定位, 此时无人机重要的功能是要保证不漂移出去, 或者局部悬停, 只需要里程计的 相对于某个点的 局部定位即可, 即无人机相对于这个点不运动

# liosam在室内狭窄环境 需要把odometrySurfLeafSize mappingCornerLeafSize mappingSurfLeafSize全部调小, 否则会因为voxel的粒度太粗导致无法限制住imu的抖动, 从而造成累计误差越来越大, 然后导致imu优化出错误的ba和bg, 最终导致轨迹发生大幅度漂移,建图系统崩溃

# 与fast-lio相比, liosam对外参标定和时间同步的要求更低, 有一个大概的雷达-imu的外参就可以给lidar提供比较好的初始解; 但是可能需要比较好的imu; 而fast在外参和同步较差的情况下, 面对大的运动效果很差; 

注意 fast lio使用yesense imu测试时, 当imu的频率调高的时候, 注意对应的ac_cov和gyr_cov也要调低(最好和标定出来的值一致), 这样才能匹配高质量高采样率的imu

**** fast lio的一个巨坑: velodyne类型雷达的时间戳在读取的时候, 应该是把timestamp*1000, 而不是/1000

# kalibr在标定相机时, 要把time-calibration也打开, 这样才能获得比较准确的标定值;
一个验证过的传感器-标定板方案: apriltag6x6-0.088尺寸 + realsense t265出厂标定的内参 + yesense imu
!!!!!!!!!!!终于成功的实现了一次外参标定
重点:
1. 用pointgrey相机4mm镜头和livox的imu(g值9.805), 用mcu做硬件时间同步, 曝光时间5ms, 初始估计相机->imu时间差为0.009s左右
2. 在标定的过程中, 注意比较缓慢的移动相机和imu; 数据采集时长为120s左右并开启时间标定. 最后的freerun动作, 做的是从左下角旋转到右上角, 从右下角旋转到左上角; 
3. 最后验证的过程, 首先看pdf, error的直方图 加速度和角速度都在0.0几左右, error的二维分布图在0.5pix以内; 用这套外参跑vins mono在不开外参估计的情况下可以正常初始化; 用这套外参跑r2live在不开外参优化的情况下不会崩溃并报so3错误; 但是肉眼观测外参的translation部分与测量的值并不是太一致,这里需要进一步确定.
4. 实验发现vins对外参的translation部分好像并不敏感, 即使轻微改变translation部分也能正确初始化, 看来是rotation部分需要精确的标定
5. 1~4并不能保证标定出正确的外参, 实际上kalibr的标定单目imu的成功率非常低
6. 一个观察到的现象: 双目imu标定时, 如果其中某一目的初始reprojection error较高, 那么最终标定的相机-imu外参就有可能是失败的, 这个可能和相机的内参标定有关; 主相机和imu通常会标定成功, 但是从相机和imu就会标定不成功, 此时双目标定的外参是可以用来跑vins fusion的, 但是快速运动会导致崩溃, (开启在线外参估计后就不会崩溃了), 说明外参标定还是存在误差
7. 对于6, 双目鱼眼相机的标定, 从相机-imu的标定效果要稍微好一些, 其表现为vins-mono能跑的时间稍微长一点, 但是最后如果不开外参估计, 还是会发散; 而vins-fusion可以直接跑下来
# fast lio 要对原始点云做一定的下采样 才有最好的效果, 而不是用全部的点云

# 跑r2live式犯的一个sb错: livox的imu要乘以g值; kalibr标定的时候也有这个sb错误

# fast lio系的一个参数问题:
如果将voxel size设置的过小, 虽然能提高建图精度, 但是静止初始化的过程中可能会出现由于匹配点搜索范围过小, 帧间无法匹配导致的退化问题
将voxel size调大可以缓解这个问题, 但是会降低建图精度

# 双目VINS fusion叠图+运动畸变消除效果比fast li还要好???
但是双目需要同时照到廊道的地板和天花板, 此时才不会有x方向的退化; 当数据采集的后期, 双目拍摄的范围大部分只有地面, 此时VIO的纵向退化, 相应的叠图效果也变差

# 运动先验对VSLAM系统的影响
视觉静止的先验模型：SVO 、LSD-SLAM
匀速运动的先验模型：ORB-SLAM、PTAM、DSO
在视觉惯性联合初始化之后，IMU就可以光明正大地参与运动跟踪了。
在VIO中，通常都会拿IMU作为运动先验，因为在联合初始化完成之后，陀螺仪和加速度计的偏置都可以被估计出来啦，而且重力矢量也可观测。此时的IMU除了一些小的测量误差以及器件的随机游走之外，基本上是没什么其他误差的。由于两帧图像之间的时间很短，其间的传感器偏差可视为定值，通过IMU测量数据预积分，可以在新一帧图像到来之前对运动进行先验估计。
SVO原程序中，采用的是位姿变化为0的先验模型，也就是静止模型：将上一帧位姿作为当前帧的位姿先验，进行图像匹配。这种方法虽然在能动性上差了一点但是也是有好处的，比如在运动缓慢的情况下、或者静止的情况下，视觉静止模型就表现得非常稳定。
而反观IMU作为先验，虽然在机体快速运动过程中IMU的测量精度很高，但是也要考虑缓慢运动状态下的IMU漂移可能引入累积误差的情况。
出于这一点考虑，我认为将IMU先验模型与其他静态更稳定的模型进行组合，再作为运动先验是一个比较合适的想法。

## 怎么选择一个更好的先验运动模型
数据集中一般都提供了groundtruth，使用其中的姿态四元数以及位移矢量作为真值。但是要验证运动先验额准确性的话需要基于上一帧的位姿，为避免位姿估计误差的影响，可以采用计算单帧运动增量的方法验证先验模型。
通过先验模型估计运动增量，再与真实增量求绝对误差，通过整条运动轨迹求单帧先验误差可以得到关于先验模型误差的对比曲线。

# SVO stereo在静止的时候可能会recompute imu, 导致崩溃

# VINS在跑的时候, odom是相对于world系下的, 第0帧是在重力系下; 如果想要相对里程计, 需要取后续里程计相对于第一帧位姿; 否则应用于2d导航会有问题

# OpenVINS 在使用较差的IMU, 做2D平面内的平稳退化运动时, 最好将在线估计相机内参的功能关闭, 否则退化运动会使得相机内参估出现发散, 尤其是顶视相机的情况, 会导致z轴出现较大的偏移, 并且可能在静止的时候由于内参和ba/bg的耦合漂移导致整体位姿出现较严重的漂移

# 目前来看T265内的算法是类似OpenVINS实现的MSCKF + ZUPT + 调大max_slam值

# 跟内外参标定没关系，标定不准是你的基本功问题，搞不定就别玩视觉了。当然除非遇到很不标准的镜头，产生不规则的畸变，这个我也遇到过。
你理解的图像质量就是亮暗程度？不同车库的地面反光情况差别、光源的光谱差异、CMOS的解像度、车大灯造成的光晕、自动曝光策略造成的过曝欠曝、flicker、紫边、拖影、果冻效应、4个摄像头的同步性，太多因素了。好的视觉产品70%取决于摄像头及其ISP控制，算法占很少。
不要以为光靠增加数据量就可以，何况你说的还是数据增强而不是增加数据，本质上没增加信息。你增加的大多数数据都不是corner case，对提升泛化能力没有任何帮助。在对回归精度要求不高的场合，泛化能力也许能凑或用，比如只做个目标检测，对角点坐标回归精度要求不高，但是做到自动泊车（包括AVP-SLAM里面这种要用角点点云做match），那对车位的定位精度要求是像素级别的，差一点车就进剐蹭了，这还不包括你的算法可能给出很多ghost目标。
小鹏p7的自动泊车你试过吗？还是看的demo？你怎么知道他用的是超声波雷达还是视觉，他可以用超声波雷达检测车位然后告诉你用的是视觉。并且小鹏那个没有建图的过程，最多做了个车位检测，比目标检测也高明不了多少。

# 一个比较有趣的对点云parameterization方法: 用1分类-分类器比如SVM, 训练的时候, 输入是点云坐标, label是正例; 输入是任意与点云距离远的坐标, label是负例; 训练之后,用模型参数作为点云表征

# 论文里比较重要的是文字大小统一, 具体做法
1. 在论文里关闭figure缩放, 而是用pyplot来控制图的绝对大小
2. 图片的width改成3.5inch左右, height随内容改变, 这样可以保证比例的一致性
3. 字体渲染使用tex方式, 这样可以使用和tex一致的衬线字体  

# 图像提特征点, 最好先分成tiles, 然后每个tile上单独提特征, 最后再合并在一起, 这样可以让特征点尽量平均的铺在图像上, 减少特征点集中的情况


# VINS-FUSION 特征点太多的情况也会导致VIO精度下降
# 加噪声之后, 发现树的特征重复性比较明显, 而路牌比较有代表性, 因此在去全局配准的过程中, 检测路牌, 增加路牌对应点云的权重

# 预积分的作用: 本身是一个独立的值, 由于他的特殊计算方式, 在优化更新R_w和b_a/b_g之后, 不用重复计算预积分过程, 可以直接得到预积分这一项的增量, 即做一次标量加法就可以了

# liosam gps的用法: ekf_localization需要三个源: gps,imu,和odometry, 由于liosam本身有个lio, 所以就用lio作为odometry的源(发布的应该是tf的形式);如果按照liosam的配置, 不运行lio, 那么ekf_localization就一直收不到odometry, 就会导致/odometry/gps一致在0点


# ESKF理解
# EKF在传播时求状态的状态转移矩阵用到了线性化，而ESKF是对误差状态的转移矩阵进行线性化；由于误差状态的线性性更强，所以明显ESKF在此处更好
1. 首先用传感器测量（或者任何不需要估计直接可以得到的量）来获取到下一时刻state的先验
2. 然后需要求解的是error state(而不是像传统的KF一样直接求解state)
3. 用先验state加上error state，就是最终要求解的值
4. 通常在运动学方程中, g都是表示在inertial frame下
5. 疑问，为什么实现中求H的过程都不带有d(x)/d(delta x), 而都只是像EKF一样只用了d(h(x))/d(x)? 一个猜测是delta x中的delta theta总是小量，其jacobian接近I，而d(x)/d(delta x)其他部分也是， 这样d(x)/d(delta x)整体就是I矩阵
6. ESKF和EKF的过程类似，就是传播的过程中多了一步对error-state的传播，在更新过程中H阵是观测对error-state的雅克比，更新过程对error-state进行更新

# IEKF
IEKF的求解过程
1. 定义目标函数f(x) = z - h(x), 就是KF中观测量与预测量处观测方程之间的差值
2. 然后用GN方法最优化f(x), 这样最好理解

# LINS使用的是robocentric frame的表征方式， state里保存的位置始终是相对于上一帧的相对位姿
We use a robocentric formu- lation to build the iterated ESKF because it prevents large linearization errors caused by ever-growing uncertainty


# 拓扑地图+局部路径规划器，这样是不需要构建精准的全局地图的

# 注意ESKF的方差传播，这就是一般线性形式的方差传播过程
x_t+1 = F * x_t
那么方差的变化就是 P_t+1 = F * P_t * F^T
所以对于state以及error-state，有了state的kinematics，就可以知道方差是怎么变化的

# 基于非流型的estimator中，每次update之后没法保证quaternion和gravity的模1性质，因此需要在每次update之后将quaternion和graviaty进行归一化

# INSIGHT: 
1. slam中translation的小变化不会造成的累计误差量级与变化相同， 但是rotation的小变化会造成非常严重的累计误差， 所以在退化环境下一个重要的事情是准确估计航向角的变化
2. KITTI Odometry这一评价指标，是将每个小段的轨迹Segment都转换到原点，然后在不对齐的前提下来计算位姿点的距离。这样其实比较接近odometry的应用场景，就是在任意点主要定位方式失效之后切换成odometry，此时看的就是odometry相对于失效点的相对位置误差随距离的累积


# 一个好算法：
1. 对初始值的依赖应该非常理性，初始值给的越好算法的效果应该越好
2. 对数据质量的依赖应该非常理性，数据量越大效果应该越好
3. 对外部参数稳定，外部参数很小的变化不应该导致算法效果很大的差异

# 采样对于detection和slam的重要性
以LiDAR Point Cloud为例， 同样的N个点，如果采样选择的不好， 不好的采样算法可能采集的都是同一条SCAN上的点， 而好的采样方法(FPS等)可能采样到的就是一个局部的平面之类的特征， 这明显会更具有判别性(无论是点面匹配还是局部区域特征抽取) 

# VAE->GAN->Diffusion

### 生成模型的本质目的: 是求出X的分布, 这样就可以从X的分布p(X)中随机采样, 采出来的样本就是符合X特性的. 但是现在不可能求出X的分布p(X), 但是可以从一个已知的比如标准正态分布N(0, 1)中采样出Z(称为隐变量), 然后学出来Z到X的映射, 这样得到的X也相当于从X的分布p(X)中随机采样出来的.

### 生成模型的难度在于判断生成分布与真实分布的相似性, 对此, 
		- GAN的思路是直接学习出一个判别器, 在理论上通过大量的样本来判断两个分布是不是相似(而生成器的能力只是能够从噪声去生成一个图像, 这个图像的perception loss(经过判别器这encoder再经过l1得到的loss)尽量低)(GAN没有机制来将隐变量Z分布的样本和Xk进行关联, 所以这种关联关系都是硬学出来的, 而学习的监督信息是判别器给的. 而VAE的机制直接确定了隐变量是和Xk关联的)
        - 而VAE的思路是使生成器直接具有这样的能力: 从样本的角度来讲, 将某个Xk对应的随机正态分布Z分布中采样得到样本, 转换回X的分布p(X)下的样本Xk的能力; 而从分布的角度讲,就是"将不同的先验正态分布转换成同一个(我们最终要的)分布,即X的分布p(X)的能力". 如果生成器直接具备这样的能力, 那么得到的分布一定是与真实分布相似.(这样VAE保证了在训练过程中, loss设置成生成图像尽可能接近原图就可以, 其生成新东西的能力是由Z是随机采样(保证了从分布转换为分布, 而不是从样本转换为样本, 因为从Z中随机采样就代表着对于Z中所有的样本都要满足这个能力, 那么也就是对于整个Z的分布都满足这个能力)这件事情和其数学原理决定的, 而不需要像GAN一样需要一个判别器)


### P(Xk|Z)用函数理论来解释, 就是每一个Xk 和 全体Z可能的数值之间的对应关系, 在计算过程中从, 样本的角度来讲, 其函数中某一个点的意义是在某个Z出现的情况下Xk的取值. 在VAE中, 任意一个Xk都有独特的这样一个分布函数


其实，在整个 VAE 模型中，我们并没有去使用 p(Z)（先验分布）是正态分布的假设，我们用的是假设 p(Z|X)（后验分布）是正态分布。

具体来说，给定一个真实样本 Xk，我们假设存在一个专属于 Xk 的分布 p(Z|Xk)（学名叫后验分布），并进一步假设这个分布是（独立的、多元的）正态分布。

为什么要强调“专属”呢？因为我们后面要训练一个生成器 X=g(Z)，希望能够把从分布 p(Z|Xk) 采样出来的一个 Zk 还原为 Xk。

如果假设 p(Z) 是正态分布，然后从 p(Z) 中采样一个 Z，那么我们怎么知道这个 Z 对应于哪个真实的 X 呢？现在 p(Z|Xk) 专属于 Xk，我们有理由说从这个分布采样出来的 Z 应该要还原到Xk 中去。

事实上，在论文 Auto-Encoding Variational Bayes 的应用部分，也特别强调了这一点：

论文中的式 (9) 是实现整个模型的关键，不知道为什么很多教程在介绍 VAE 时都没有把它凸显出来。尽管论文也提到 p(Z) 是标准正态分布，然而那其实并不是本质重要的。

再次强调，这时候每一个 Xk 都配上了一个专属的正态分布，才方便后面的生成器做还原。但这样有多少个 X 就有多少个正态分布了。我们知道正态分布有两组参数：均值 μ 和方差 σ^2（多元的话，它们都是向量）。

### 每个样本Xk对应的Z的先验分布是不同, 而我的生成器,具备"将不同先验正态分布转换成同一个(我们最终要的)分布,即X的分布p(X)的能力" (从样本的角度来讲, 是"将某个Xk对应的Z分布中采样得到样本, 转换回X的分布p(X)下的样本Xk的能力"), 我们在实现上是通过的训练算法和数据集让模型有这样的能力, 这才是VAE真正要学到的东西. 这也是我们能做"每个Xk对应独特的Z分布"的前提条件.

### 这样如果我们想生成一个新的未知X, 可以直接自Z空间里随机采样, 这个采样可能是来自于任意已知或者未知Xk所对应的Z分布(但是数值上他就是一个随意的n维向量), 而我们的生成器, 恰好具备"将某个Xk对应的Z分布中采样得到样本, 转换回X的分布p(X)下的样本Xk的能力", 这样通过我们的生成器转换为X真实分布下的一个采样. 这在数学上是合理的.

那我怎么找出专属于 Xk 的正态分布 p(Z|Xk) 的均值和方差呢？好像并没有什么直接的思路。

那好吧，我就用神经网络来拟合出来。这就是神经网络时代的哲学：难算的我们都用神经网络来拟合，在 WGAN 那里我们已经体验过一次了，现在再次体验到了。

于是我们构建两个神经网络 μk=f1(Xk)，logσ^2=f2(Xk) 来算它们了。我们选择拟合 logσ^2 而不是直接拟合 σ^2，是因为 σ^2 总是非负的，需要加激活函数处理，而拟合 logσ^2 不需要加激活函数，因为它可正可负。

到这里，我能知道专属于 Xk 的均值和方差了，也就知道它的正态分布长什么样了，然后从这个专属分布中采样一个 Zk 出来，然后经过一个生成器得到 X̂k=g(Zk)。

现在我们可以放心地最小化 D(X̂k,Xk)^2，因为 Zk 是从专属 Xk 的分布中采样出来的，这个生成器应该要把开始的 Xk 还原回来。于是可以画出 VAE 的示意图：


让我们来思考一下，根据上图的训练过程，最终会得到什么结果。


首先，我们希望重构 X，也就是最小化 D(X̂k,Xk)^2，但是这个重构过程受到噪声的影响，因为Zk 是通过重新采样过的，不是直接由 encoder 算出来的。


显然噪声会增加重构的难度，不过好在这个噪声强度（也就是方差）通过一个神经网络算出来的，所以最终模型为了重构得更好，肯定会想尽办法让方差为0。

而方差为 0 的话，也就没有随机性了，所以不管怎么采样其实都只是得到确定的结果（也就是均值），只拟合一个当然比拟合多个要容易，而均值是通过另外一个神经网络算出来的。

说白了，模型会慢慢退化成普通的 AutoEncoder，噪声不再起作用。

这样不就白费力气了吗？说好的生成模型呢？

别急别急，其实 VAE 还让所有的 p(Z|X) 都向标准正态分布看齐，这样就防止了噪声为零，同时保证了模型具有生成能力。

怎么理解“保证了生成能力”呢？如果所有的 p(Z|X) 都很接近标准正态分布 N(0,I)，那么根据定义：

这样我们就能达到我们的先验假设：p(Z) 是标准正态分布。然后我们就可以放心地从 N(0,I) 中采样来生成图像了。

### 在VAE的实现中, 还加了约束, 使得所有Xk专属的Z分布都接近N(0, 1), 这样综合起来之后, 最终整体的Z分布P(Z)就也是N(0, 1), 理想情况下这样就保证Z的分布又回归到最原始的假设上了, 而我们生成器的能力, 又变回"将N(0, 1)分布转换为X的真实分布p(X)". 当然在实际情况下, 所有分布肯定实在理想的N(0,1)与Xk专属正态分布之间的, 所以广义上将生成器的能力还是将多个专属分布转换为一个统一真实分布. 而模型的性能和生成能力也就由分布与N(0,1)的接近程度相关.

### 为了能使采样Z的过程梯度回传, VAE还使用了重参数技巧, 这个和连续强化学习的末端动作采样是一致的.

### 对于conditional-VAE, 以类别标签生成图像为例子, 就是让某一类别的Xk专属的Z分布都尽量接近N(类别均值, 1), 这样在推理的时候, 通过生成"接近类别均值的Z", 就可以让模型生成接近对应类别的样本.

### Diffusion和VAE思路类似, 但是从样本Xk生成正态分布Z的过程的均值方差不是学出来的, 而是通过多次加高斯造成Difuse出来的, 每一步生成一个正态分布噪声结合上一步的Xk输入, 最终能够得到完全服从正态分布的噪声Z(也就是我们要的隐变量, 也就完成了样本到隐变量的转换); 而逆过程(生成)的过程, 每一步都使用了网络参数来预测每一步的加入的噪声, 从而Xt到Xt-1恢复数据(减掉噪声即可), 在最后一步去掉了所有的噪声之后就得到了原始分布中的原样本Xk. 训练过程中, 逆过程的参数的学习监督信息是使得预测的噪声尽量接近实际在前向过程加的噪声. Diffusion的逆过程(生成)更严格的符合分布变换这一数学原理,而不是简单的拟合函数变换, 实现这一原则的前提是前向过程使用了解析的方式来将分布p(x)变换为正态分布.



#### OPENAI API KEY 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


# 在车库里建图， 目前来看AVIA的Z轴是最稳定的， 这可能说明在有天花板的场景中Z方向FOV对限制Z轴飘移有好处，同时AVIA需要解决的问题是横向FOV受限制，这可能需要通过加一个横向旋转装置来解决
# FAST-LIO的det range对地图一致性影响较大
# liosam用的是9轴的IMU的姿态值作为初始姿态，而一般9轴就是与重力对齐的，所以LIOSAM初始就是和重力对齐的，其z轴的限制对于2Drobot也就更有效
# LIOSAM回环的时候会加mtx 这会在回环负载高的时候出现严重的race condition问题，会丢消息
# 建图时距离远是为了增加里程计鲁棒性；最后构成地图时保留近距离点云是为了最终地图质量


RVIZ中 flat square是有size的 近大远小 所以在可视化map的时候效果较好 远处的被遮蔽的同时近处面积较大可见性更好
而Point方式 远近都是一个pixel大小 完全没有遮挡关系

# 在BALM类算法中，surfel的timestamp可能是一个好的度量， 比如同一个墙的两面这类场景，由于常识，LiDAR不可能同时扫到两个面，因此用timestamp可以防止两个面被划成同一个面（但是对于不同时间扫到同一个面的情况可能需要单独考虑）

# Voxel Map 即使不加covariance权重 都可以有效的提高LIO的鲁棒性 0.25Voxel Size
# VoxelMap的原始实现中 没有对BA求导
# hilti2022 03序列 调低IMU的权重 可以提高楼梯间的鲁棒性
# 估计外参的公式是正确的 但是在degenerate情况下观测量太少 效果反倒不好
# 显式的估计外参，可以允许我们根据LiDAR的文档更准确的设置ranging cov以及angle cov，所以可能导致了更好的效果
# 动态物体的马氏距离