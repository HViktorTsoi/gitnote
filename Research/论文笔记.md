# Faster RCNN
Faster RCNN中RPN的Anchors原理解析：

得到60*40的feature map之后，对于map上的每一个像素，分别由

- 1x1x18(9 * 2)卷积生成每个像素的9个Anchor的分类信息，即该Anchor是否有目标

- 1x1x36(9 * 4)卷积生成每个像素的9个Anchot的偏移信息(Dx,Dy,Dw,Dh)，这个信息是用来加到固定的Anchor上的，也就是说，在配置中，我们固定了Anchor有3个大小，3中尺度，共3*3=9个Anchor，但是这种固定大小的肯定是和GroundTruth的大小不能完全匹配，而且负责生成这个Anchor的像素的位置和GroundTruth的中心位置也不能匹配，因此这第二个1x1卷积就是用来尽量的学到固定大小的Anchor和GT之间的大小的位置差距，从而使Proposal更加准确(当然没有Stage 2 的回归更准确，这里只是说相对的准确)。

其中，固定的基准Anchor是在训练一开始，初始化模型的时候就一次性生成好了，而不是每个像素进行1x1卷积时才生成的，这个固定的基准Anchor+RPN预测的偏移值，就得到了Proposal。

下边这段存疑,RPN生成的proposal应该就是anchor到GT Box之间的偏移,在求RPN_CLS_LOSS和RPN_PRED_LOSS时求的是anchor+proposal偏移之后和GT Box之间的差距.
> 在给每个像素生成固定Anchor之后，且在固定Anchor加上1x1conv的偏移之前，就需要求RPN_CLS_LOSS和RPN_PRED_LOSS，通过ground truth box与固定的基准anchor之间的大小和位置差异来进行学习，从而使RPN网络中的权重能够学习到预测box的能力。(这里没有求GT和"已经加了偏移的Anchor"之间的差距，从而进行学习的原因是，因为网络在每一轮训练或者预测开始时，只能获得到固定基准Anchor的大小和位置，无法获得上一时刻已经加了未经修正的偏移的、更准确的Anchor位置，因此实际上RPN要学的是基准Anchor和GT之间的差距，让这个差距越来越接近真实的差距，而不是学怎么预测出一个框使得这个框和GT特别接近，如果是这样的话Anchor就没有意义了)。

因此这里Anchor的作用就是，给RPN提供一个回归的基准，如果Anchor本身就很接近大部分目标的形状了，那么RPN就只需要很少的学习量，能够很好的提升学习性能；如果只用1个Anchor，或者不用Anchor，或者基础Anchor的大小和GT相差很大，那么RPN就需要很大的功夫去学习生成的Proposal和GT之间的偏差，甚至可能不收敛。


# Cascade RCNN
# FPN
# mmdetection
Unet系列用于semantic Segmentation
Mask RCNN系列用于instance Segmentation

对于mmdetection，当anchor_base_sizes为None时，使用anchor_strides充当anchor_base_sizes,然后用fpn每一层的map使用对应anchor_base_size，结合所有的anchor_scales和anchor_ratios生成框，进行前后景的分类。

anchor_ratio主要决定anchor的形状即宽高比；

anchor_base_sizes * anchor_scale生成所有的base_anchor；

然后base_anchor * anchor_strides生成各个feature_map上使用的anchor。

在进行anchor assign时，p2 ~ p6 在anchor_stride对应的5个值生成的所有anchor坐标是拼接在一起的，生成一个(p2+p3+p4+p5+p6,4)的tensor

mmdetection在config的backbone中可以直接配置dilation等参数
 
# HTC
> 这篇文章的Experiment里提到了很多COCO上涨分的技巧,可以学习一下.

![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/05/02/1588353289307-1588353289336.png)

- Cascade RCNN: 级联bbox head, 每一级预测的bbox输出作为下一级的输入,迭代refineb box.但是cascade没有考虑不同stage之间mask的信息传播,也没有考虑bbox分支和mask分支,强行在每一个stage 里面塞下了两个分支，但是这两个分支之间在训练过程中没有任何交互，它们是并行执行的;

- Interleaved execution: 每个stage先执行bbox回归,将回归之后得到的框交给mask分支,让mask分支能用到bbox的检测结果;

- Mask Information Flow: 这一步起到了很重要的作用，对一般 cascade 结构的设计和改进也具有借鉴意义。我们首先回顾原始 Cascade R-CNN 的结构，每个 stage 只有 box 分支。当前 stage 对下一 stage 产生影响的途径有两条：（1）$B_{i+1}$ 的输入特征是$B_{i}$ 预测出回归后的框通 RoI Align 获得的；（2） $B_{i+1}$ 的回归目标是依赖 $B_{i}$ 的框的预测的。这就是 box 分支的信息流，让下一个 stage 的特征和学习目标和当前 stage 有关。在 cascade 的结构中这种信息流是很重要的，让不同 stage 之间在逐渐调整而不是类似于一种 ensemble。
然而在 Cascade Mask R-CNN 中，不同 stage 之间的 mask 分支是没有任何直接的信息流的， $M_{i+1}$ 只和当前 $B_{i}$ 通过 RoI Align 有关联而与 $M_{i}$ 没有任何联系。多个 stage 的 mask 分支更像用不同分布的数据进行训练然后在测试的时候进行 ensemble，而没有起到 stage 间逐渐调整和增强的作用。为了解决这一问题，我们在相邻的 stage 的 mask 分支之间增加一条连接，提供 mask 分支的信息流，让 $M_{i+1}$ 能知道 $M_{i}$ 的特征。具体实现上如下图中红色部分所示，我们将 $M_{i}$ 的特征经过一个 1x1 的卷积做 feature embedding，然后输入到 $M_{i+1}$ ，这样 $M_{i+1}$ 既能得到 backbone 的特征，也能得到上一个 stage 的特征。
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/05/02/1588355070046-1588355070047.png)
- Semantic Feature Fusion: 这一步是我们尝试将语义分割引入到实例分割框架中，以获得更好的 spatial context。因为语义分割需要对全图进行精细的像素级的分类，所以它的特征是具有很强的空间位置信息，同时对前景和背景有很强的辨别能力。通过将这个分支的语义信息再融合到 box 和 mask 分支中，这两个分支的性能可以得到较大提升。
具体实现上,是在FPN后边加上一个简单的FCN(不是UNet那种Encoder-Decoder,就是一个straight-down的FCN),首先将 FPN 的 5 个 level 的特征图 resize 到相同大小并相加，然后经过一系列卷积，再分别预测出语义分割结果和语义分割特征。训练时的监督信息使用COCO Stuff全景分割数据集(这里在应用Semantic Feature Fusion的时候要考虑,自己的数据集里是否有这样的全景label).
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/05/02/1588355181545-1588355181546.png)



# PointNet
http://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf

# PointNet++
http://papers.nips.cc/paper/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space.pdf
MSG：取不同半径的点的特征拼接在一起
MRG：高层少数点特征pool+低层对应这些少数点的多数点特征pool拼接
无序主要影响的是没有局部信息

# SehllNet
http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_ShellNet_Efficient_Point_Cloud_Convolutional_Neural_Networks_Using_Concentric_Shells_ICCV_2019_paper.pdf

# Octree guided CNN with Spherical Kernels for 3D Point Clouds
https://arxiv.org/pdf/1903.00343.pdf

# PointCNN
邻居点feature和邻居点xyz提取的feature拼接，由于不同邻居点的顺序对生成不同的点集，学习一个排列矩阵，将邻居点顺序进行重新排列，从而实现点顺序的无关性质

# Deep Snake
Deep Snake Head：
1 输入是轮廓的N个点
2 经过多次circle conv，生成轮廓feature
3 进行1次1x1 conv，再max pooling，生成整个轮廓的全局feature(这里为什么用max pooling？汇集全局信息应该用avg pool啊)
4 再进行多次1x1 conv，得到最终的每个轮廓点的offset
用轮廓作为分割边界
1 首先生成detection的bb
2 选择bb四个中点，生成菱形
3 过一次Deep snake 生成物体的四个角点的offset(为了丰富边缘特征，实际上输入的40个点对应的feature，每个边被插值成了10个点，但是最后loss只负责监督输出的角点到bb的四角的offset)
4 分别以四个角点为中点，扩展出四条水平/垂直线(这四条线实际上就位于bb的四条边上)，其长度是bb的1/4；然后再把这四条线的两边连起来，最终构成一个八边形。这个八边形就比原来的矩形bb更好的框住了物体的轮廓，就把这个八边形作为snake的初始轮廓。
5 将这个初始轮廓采样成N个点，再送进Deep Snake，得到最终轮廓的N个offset(N一般是128)
6 重复i次步骤5(生成offset后，对原来的轮廓进行deform，然后将新轮廓采样之后再送入deep snake)，因为有的点离GT过远，需要反复进行偏移才能到GT附近
7 对于被遮挡物割裂的物体，使用multi component detector，先对Feature map做roi align，然后用一个多组件detector来检测目标，分别对每个组件做snake，最后再将轮廓合并

# Adder Net
把原来求互相关的conv转化为求L1-Norm(差的绝对值)

在bp过程中，原本的导数是sgn(X-F)，但是为了训练稳定，改为X-F，更精确；但是这个值可能太大(大于原本的+-1之间)，因此对与往前一层传的导数还要进行clip到+-1之间，使优化更稳定。

由于Adder conv的输出方差更大(见原论文)，需要借助BN来处理其输出；但是由BN的反传公式，方差越大，在BN层往前传的梯度越小，学不动了，因此需要适当增大学习率；但是通过实验发现，不同层的梯度大小分布也不同，因此不能对所有层的学习率都增大相同的倍数，而是使用了a~l~这个可调整系数，它与对应参数梯度的l2-norm成反比，也就是梯度越小，学习率相对更高，这样保证网络的不同层参数在优化时的实际更新的梯度是接近的。

# YOLO v4
## Bag of freebies
### 数据增强
随机裁剪旋转；
Random erase，CutOut；
DropOut；
MixUp，CutMix；

### 数据分布
hard example mining;
focal loss;
label smoothing;

### BBox regression
普通的BBox regression(回归角点，回归到anchor的offset)；
IOU Loss，GIOU Loss，DIOU Loss，CIOU Loss；

##  Bag of specials
### 增加感受野
SPP，ASPP，RFB；

### Attention
SEBlock,SAM;

### Feature integration
FPN,SFAM,ASFF,BiFPN;

### Activation
LReLU,PReLU,ReLU6,Swish,Mish;

### Post processing
Soft NMS；

## Backbone
需要感受野大，并且网络参数大，且输入图像够大
CSPDarknet53 + SPP + PANet + YOLOv3 Head

## 数据增强
Mosaic：就是大小为4的CutMix，找四张图像分别随机裁剪一小块下来，然后拼接在一起。这样可以让网络在被feed进一张图像时，接触到更多种类的contex的信息；另外就是BN在计算分布的时候用到了4张图像的统计信息，就不需要更大的batch size。

SAT：Self-Adversarial Training，循环执行两轮，第一轮固定网络参数训练扰动，目标是使检测结果达到最差；第二轮固定扰动正常训练网络参数，目的是在有扰动的情况下让检测结果达到最好
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/04/24/1587724175754-1587724175834.png)

CmBN：modified CBN，将1个batch划分为4个mini batch，对每一个mini Batch，分别accumulate W，再accumulate 这个mini batch对应的BN(传统的BN是accumulate这个大的batch中所有已计算的mini batchBN)，然后normalize BN，但是不更新W和BN的参数，直到最后一个mini batch算完之后再更新参数。

modified SAM：Spatial Attention Module
modified PAN：把add换成了concat


# Grid-GCN for Fast and Scalable Point Cloud Learning
Liu et al. [26] show that the data structuring cost in three popular point-based models [22, 45, 40] is up to 88% of the overall computational cost. (with Code)

提出了Grid-GCN，Coverage-Aware Grid Query (CAGQ).
1. 首先voxelize；
2. $O_v$是所有非空voxel，从中采样M个中心voxel $O_c$；
3. 因为已经构建了point-voxel的对应关系，就可以直接从映射表中查询某个中心voxel的邻居voxel（称之为context points）；
4. 从context points中随机采样K个点，计算这K个点的重心，作为这个点分组的group center；
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/04/29/1588099942606-1588099942631.png)

5. 这里有两个问题，第一个是如何选择中心voxel $O_c$，文章给出两种方法：
	1. RVS，纯随机采样
	2. CAS，Coverage-Aware采样，保证选择的点能最好的覆盖到整个空间，能覆盖到点最多的区域。具体做法是，先使用RVS随机采样M个点作为incumbent $V_I$；然后对于所有没被选择的非空voxel，依次作为challenger $V_c$，通过以下的式子(1~3)计算用$V_c$代替$V_I$所获得的覆盖率增益，如果这样做了之后能提升覆盖率，那就执行代替操作。
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/04/29/1588093912391-1588093912392.png)
- 在这个式子中，$C_V$代表点V的邻居voxel是incumbent的数量，$\lambda$代表邻居voxel的数量。
$H_{rmv}$代表删除incumbent后减少的覆盖率，其含义就是，对于$V_I$的某个邻居V，如果V的邻居中除了$V_I$之外还有其他的incumbent，那么$C_V-1$的值一定大于0，则$\delta$就是0，这说明删除了$V_I$之后，他的附近还会有别的点来代表他，因此覆盖率不会减少；反之，如果$V_I$的邻居的邻居都没有incumbent，这说明$V_I$附近都没有代表点，那么删除他之后这个区域的覆盖率就会受到影响而降低。
- 对于$H_{add}$，只看第一项，和$H_{rmv}$是一个概念，如果$V_C$邻居的邻居没有任何incumbent，说明添加这个点会增加覆盖率，反之也成立；而第二项是一个正则项是防止过度覆盖的，就是防止对$V_C$的选择过于宽松，导致最后过多的原incumbent被替代，结果采样的点过多。
6. 如何采样这K个点，同样有2种方法：
	1. cube query，类似于ball query，但是在点云密度不均匀的时候可以覆盖更大的空间。
	2. KNN。和全局KNN不同，这个KNN只需要搜索context voxel就可以。
7.  Grid Context Aggregation：每个点的特征由以下组成：
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/04/29/1588100887511-1588100887513.png)

	1. Coverage Weight，就是group center所连接的node的数量(这里有个问题，前边说了pick K node，难道这个weight不总是k吗？)；
	2. Grid Context Pooling，就是说不把中心点看做一个实际的点云中的点，没有feature（实际表现应该为，不存储这个中心点的feature到query tree中），而是在计算下一层的特征时，对group center的所有node的Feature做pooling得到一个feature向量；
	3. Edge attention,像pointnet++这类的网络没有考虑edge attention，如果pointnet2的group也看做一个图的话，那么就是中心点的所有edge连接的邻居的feature经过mlp之后，直接得到映射之后的feature，每个邻居(或者说edge)的权重是相等的；而edge attention则是给每一条edge再单独学出来一个权重，然后乘到mlp之后的feature上，从而体现出在图中不同邻居的重要性的不同。(这里有点像SEBlock，只不过SEBlock是在通道上的权重，而edge attention是在邻居节点这个维度上的attention)；
	4. Aggregation，最终对于点i，它的特征学习对应的输入是它的feature vector；它的edge attention对应的输入是：group center的坐标，点i的坐标，点i的coverage Weight，点i的feature vector(文章中称之为semantic)，以及Grid Context Pooling(即pooling之后得到的grouping center的feature)。在学习到下一层的feature和edge attention之后，将feature vectore和edge attention相乘，得到最终的feature。

 
# Federated Learning

## 优势:
- 网络带宽优势,只需要传输网络参数,不需要传输raw data
- 隐私性保护
- 低延迟,本地模型可以直接在本地做inference

##局限性:
- 网络带宽仍然是一个限制
- 边缘计算网络中各个设备的异质性
- model 参数中仍然有可能包含数据信息,因此隐私性仍然难以得到保护

FL的经典算法是FedAvg,就是端设备将学习完的参数传送到server之后,server把所有的参数按照子集大小加权平均下来,得到全局的最终权重

## FL framework
FL协议包括以下几个阶段:Selection,Configuration,Reporting.
Fl框架
- TensorFlow Federated (TFF)
- PySyft
- LEAF
- FATE

# FlowNet3D
# method
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/05/12/1589292505804-1589292505859.png)
pointcloud的光流计算难点:假设场景为刚体,并且需要找到对应点(这里是不是可以考虑few-shot learning)

1. 首先用pointnet++来提取点云特征
2. 在求光流时,首先对于前一帧的某个点的neighbor区域,找到后一帧也落在其中的点(有很多个)
3. 然后对这些点做一次卷积+max pooling,vote出和前一帧这个点对应的点(具体怎么vote这个是学习出来的)
4. 注意,另一求对应点的方法是,对于某个点,直接求其feature vector和目标帧neighbor中点的feature vector的距离,找到距离最近的那个点,但是作者证明这种方法没有3好.
5. flow插值:没用3d插值,而是对unknown set中的每个目标点,用knn或者ball query在原来的点少的known set中找到邻居点,然后过pointnet的conv,得到目标点的feature.作者通过实验说明这种上采样比原来的3d interp效果好.
6. Loss分两部分,一部分是SmoothL1 loss;另一部分是半监督的loss,即将目标帧的点云用预测的flow变换回源帧,然后加约束,让变换生成的源帧和真正的源帧相近.
7. 测试的时候使用了TTA,对点云进行多次重新sample,并重新inference,取多次平均

# dataset
FlyingThings3D, KITTI

# Depth Completion via Deep Basis Fitting
## 用lsf layer代替了FCN中的1x1卷积，来生成稠密的深度图；
思路是这样的：
在1x1卷积中，卷积核的参数是在训练过程中由监督信号(稠密深度)指导反向传播生成的；
而在lsf layer中，这个参数是直接由最小二乘回归得到的，回归的输入是由fcn提取的Feature map $B$，而这里回归的输出要注意，是稀疏深度图$S$(不是稠密深度图)，在计算回归的过程中，只选择$S$中的有效深度像素参与计算，这样即使在无监督(没有稠密深度)的情况下，也能够获取到足够多的feature map pixel - sparse depth pixel对，从而也能计算出最后一层的参数。
回归的目标函数：
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/07/1591467748535-1591467748557.png)

## 应用了M-estimator来实现Robustified Nonlinear Fitting

##使用了多尺度的深度图生成器
最终生成的深度图是将不同层级的decoder产生的B经过lsf之后得到的深度图相加在一起得到的。这个类似小波变换，不同层级的深度图的频率是不同的，加在一起合成了有丰富细节(频率分布的)深度图。


# Depth Completion from Sparse LiDAR Data with Depth-Normal Constraints
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/20/1592623863846-1592623863877.png)
1. 首先prediction net，预测出：
	1. coarse depth(loss用L2 loss)；
	2. Normal(normal的GT是通过depth的GT计算出来的，loss用的是负余弦距离，因为normal是一个向量，这里考虑为什么不用L2 loss？)；
	3. Confidence Map，代表输入的稀疏深度中每个点的可信度，可信度低的点更可能是噪声。由于这没有标注的GT，文章使用depth GT中的像素与sparse输入的差的负幂来计算这个值的GT，越大代表sparse中的点深度和depth GT越接近，也就越可信。在训练过程中confidence分支用来预测这个值，loss使用L2；

2.  plane-origin distance space. plane-origin distance P实际上就是某个点X的坐标向量和normal的内积，这个值对于所有在切平面的点都应该是相等的(x是切面里的点，X是数据点，显然切面里的点的法线都是同一个法线，积也是同一个值)。

![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/08/1591547372076-1591547372077.png)
同时，根据相机的成像模型，又能得到像平面坐标x到3维坐标X的转换公式，其中D(x)是深度，C是相机内参：

![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/08/1591547386991-1591547386992.png)

将上边两个式子联立起来，就能得到P和深度的关系，这样只要能预测出P就能得到深度D(x).
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/08/1591547396147-1591547396148.png)
这里用到这一空间的目的是利用法线的信息。

![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/20/1592623907630-1592623907631.png)

3.  diffusion. 这一过程是将coarse P中的每一个像素的信息向其周围的邻居像素传播，起到类似平滑深度和深度插值的作用。这一过程使用的是各向异性传播，即在使用近邻像素n的深度来计算当前像素x的深度时，要考虑n和x的相似度，越相似元素在计算的时候加的权重越大。

![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/08/1591550561118-1591550561138.png)
那么如何度量这个相似度呢，在经典算法里，由于除了稀疏深度之外还提供了可见光图像，所以就直接用两个像素对应的图像rgb值的差距来作为相似度。而在cnn里由于可以提取高维度特征，就使用feature map(原文中称之为Guidance feature)中像素对应的feature vector的距离来作为这个度量。注意在本文里，要求的而不是深度，而是平面距离P，在2中提到，只有在同一平面上的点才有相等的P值，因此本文在diffusion的过程中关注的相似度是两个像素对应的3D点是不是在同一个平面上，由于Guidance feature也包含了几何特征信息，因此G上相似的两个点就更有可能有相似的几何特性，点也就更有可能在同一个平面上，因此这篇文章也使用feature vector的距离来作为plane-origin distance space的相似性度量。

4.  迭代优化P. 迭代过程如下：
	1. 首先计算sparse P输入和coarse P输入融合之后的P，这个公式的意思是分别在spase和coarse中选择可用的像素点对应的P值(比如sparse中的空隙，就用coarse中的值来填充)，并加上Confidence权重 ![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/08/1591550944905-1591550944907.png)
	2. 对融合之后的P执行diffusion；
	3. 将diffusion之后的P作为新的coarse，重复步骤1，直到迭代完指定的次数。
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/20/1592623944320-1592623944321.png)

5. 最后用P和法线将深度还原出来。

×× 这篇文章之所以work，是基于这样一个假定：3D物体在局部结构上是由连续的平面构成的，这样其plane-origin distance在局部也就是连续的，作者认为这个假定要比之前算法的“2D平面上深度是局部连续的”要好，因此plane-origin distance map更加符合迭代difuse来refine的特性，那么通过迭代refine P，再还原回深度，就会得到更好的深度补全结果。同时增加了normal这一个分支，也就显性的增加了法线约束这个信息流。

但是这里有问题，在difuse的过程中，计算相似度用的是feature vector的距离，作者说这个距离近的点更有可能在同一个平面上，但是没有验证这一点。

另外normal map和P也都是预测出来的，这和直接预测深度区别很大吗？

# Voxel Net (With Code)
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/17/1592363484540-1592363484565.png)

![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/17/1592365803182-1592365803184.png)

1. 划分Voxel;
2. 把点Group到voxel的代表点上;
3. 每个groupd对应的vox里进行random sample;
4. 经过vfe layer，这里和pointnet2里的基本一样，就是把每个代表点的邻域(格子里)点的feature过一遍fc，得到新的feature，然后再经过element-wise pool的feature拼接在一起，构成最终的新feature，再过一遍fc，然后再将vox里的 所有点进行avgpool，构成最终的vox feature;
5. 对feature vox进行3d卷积，最后一层在channel这个通道上reshape成2d feature map，然后经过rpn，得到proposal，对应最后的result( rpn输出的map是H/2  x W/2 x 14的，其中14代表2 x (x,y,z,l,w,h,theta) )；
6. 在实现上，为了处理sparse的点云结构，使用了和pointnet2类似的点云邻居表，其中voxel索引是pointnet2的代表点。
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/17/1592365755164-1592365755168.png)


# SegGCN: Efficient 3D Point Cloud Segmentation with Fuzzy Spherical Kernel (with Code)
主要是提出了

# Single-Stage Semantic Segmentation from Image Labels
弱监督学习方法，训练时仅给定classification label，可以生成segmentation；
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/Screenshot%20from%202020-06-19%2019-15-01.png)
1. 使用类似FCN的backbone；
2. FCN生成的feature map(K x H x W)经过1x1 conv产生Score mask(C x H x W)，这里加入的监督条件是将Score Mask做nGWP(空间上进行pooling)，形成长度为C的feature vector，然后和弱监督label(即分类label)做L1 loss；
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/Screenshot%20from%202020-06-19%2019-15-10.png)
3. 对Score Mask进行softmax，得到伪分割label；
4. 对伪label进行iterative的refine，填充分割label的孔洞 ；
5. 使用Stochastic Gate（类似drop out），防止对伪label过拟合(因为伪label是无监督生成的，过分依赖这个会造成对质量差的label过拟合)
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/Screenshot%20from%202020-06-19%2019-15-20.png)

# KP Conv

# Real-Time 3D Model Tracking in Color and Depth on a Single CPU Core
使用的就是以物体中心为原点的深度表征；使用贝叶斯手段来判断像素/深度是否属于前景/背景

# Joint Spatial-Temporal Optimization for Stereo 3D Object Tracking
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/24/1592964002336-1592964002338.png)
1. 首先提取当前frame图像的feature，然后和前一帧的feature一起送到paired RPN中；
2. 在paired RPN中，只有和两帧GT Box的并集的IOU>0.7的proposal才被认为是foreground，这样保证了正类anchor一定会cover到两帧的box，这样这个anchor既可以回归到当前帧的box，又可以回归到前一帧的box；
3. 分别对cur和prev feature map上的ROI做ROI Align，再拼接在一起；
4. RPN生成一对box(b0,b1)，分别代表当前时刻t对应的前一帧I0和当前帧I1的目标，由于2中保证anchor一定cover两帧的box，b0和b1一定关联到同一目标；而对于t+1时刻的两帧I1和I2,I1和I0中的目标是通过逐个对比IOU来关联的；I2和I1的关联方式又和I0I1相同。
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/24/1592964533849-1592964533850.png)

5. 对于3D box，将ROI Align之后的featrue进过FC之后，生成3D中心点(训练的时候学习的是ROI中心到3D box GT中心的距离，****这里有个严重的问题，有了ROI中心坐标之后不是可以直接用Velo-to-CAM投影矩阵就可以得到3D的点了吗)；3Dbox的尺寸是在coarse尺寸先验$f \frac{H_{3d}}{H_{2d}}$之上回归得到的；航向角直接是回归出来的；
6. 使用FCN获取前景object的segmentation mask；将object对应的depth转换到以object中心为原点的局部坐标系；并且预测Object图像每一个像素对应的3D局部坐标(相对于object中心)，文章说这样的表征保证同一个object的同一个part是有相同特征的；
7. 时空约束优化：
	1. 空间约束：最小化双目图像之间的重投影误差，这是用来监督稠密深度信息的；
	2. 时间约束：当前帧的前景投影到3d空间，减去当前帧的中心坐标(得到像素的相对坐标)，然后加上上一帧的中心坐标，再投影回上一帧的图像空间，这样就得到了当前帧前景在上一阵中的重投影，时间约束的目的是最小化这个重投影误差；
	3. 前景物体位姿约束：1 2都是对前景物体的相对位置进行约束，但是对于单独的一帧，为了保证检测出的目标位置是准确的，这里还要对前景物体的位姿进行约束（就是普通的Oriented 3D bbox loss）;
	4. Per-Frame Marginalization：这一步是为了更有效的利用前边更多帧的历史信息(因为1~3都是用的相邻两帧的信息)。这里实际上使用了一个8x8的参数矩阵H(8=4+4，相邻两帧$x_i$,$x_{i-1}$的[x,y,z,theta]向量拼接在一起)；对于第12帧，H是用高斯牛顿法求解约束为7.1,7.2,7.3的loss时所得到的hessian矩阵；而对于之后的所有帧，在求解目标位姿x时除了要满足7.1,7.2,7.3的约束，还要满足$||Hx_{i-1}-b||$最小，求解出这样的$x_i$之后，再用和求解第1 2帧相同的方法去更新H。这样在顺序处理所有帧的过程中，H就保留了前面若干帧的历史信息。
7. 在实验中，这个方法先在object 3D上预训练过。因为object中有一部分图像没在tracking中，用多出来这部分预训练前边的RPN和FCN，相当于扩充额外的数据集。

# ILCC
1. 先把LiDAR按照每条线的intensity反差分割成一段一段的线段集S
2. 对S中的线段进行聚类，具体做法是：
	1. 从所有剩余线段中取出第一段作为种子，放入当前cluster中；
	2. 遍历点云中所有线段，按照 (1)两线段中心距离以及(2)两线段所有点做PCA之后的协方差 作为距离，比较所有线段到当前cluster中线段的距离，将距离小于阈值的线段划分到当前cluster中；
	3. 重复1,2直到所有的线段都被划分到对应的cluster中了（有的cluster可能只包含一条线段，因为所有线段离他距离都远）；
3. 将marker从点云找出来，具体方法是先用PCA的方差判断点是不是在一个平面内，然后通过几何特性判断是不是长方体
4. 用混合高斯模型找到marker对应点云的最高和最低intensity分布的峰值对应的intensity，作为intensity二值化的pivot
5. 将marker对应的点云所在的平面拟合出来，然后把分布杂乱的点云全部投影到这个平面上；构建一个仿真的chessboard模型；将实际点刚体变换到xoy平面上
6. 使用optimizer求解旋转和偏移参数，使得在同一平面的真实点云和棋盘能够对齐。to optimizie的loss是：
	1. 如果真实点在chessboard模型的内部，判断真实点对应坐标的反射率是不是和模型一致(即真实点p在此处对应的棋盘格应该为黑色，则判断p的反射率是不是低于阈值TH，反之判断是不是高于TH)，如果不一致，则cost+=真实点到所有棋盘格的x偏移和y偏移最小值

# Feature-metric Registration: A Fast Semi-supervised Approach for Robust Point Cloud Registration without Correspondences

# Region Growing Segmentation

1. 随机生成1个seed，将seed
2. 在
