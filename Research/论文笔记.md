# YOLO v3的输出
在网络三个不同阶段输出三个feature map，大小分别是13x13， 26x26，52x52

每个feature map的通道数是(4+1+c)*k, 其中
- k为预设边界框（bounding box prior）的个数（k默认取3），
- c为预测目标的类别数，
- 4是BB参数(相对于featuremap上对应中心点的偏移XY，和相对于预设BB的宽高缩放比例WH)，
- 1是这个位置有目标的概率。 

在产生最终目标的BB时，假定这个目标在feature map上的坐标是(Cx, Cy)，预设BB大小为(Pw,Ph), fm上这个位置预测的数值在通道方向上得到的BB值是(Tx,Ty,Tw,Th)，那么最终的BB为：
- Bx = sigmoid(Tx) + Cx
- By = sigmoid(Ty) + Cy
- Bw = Pw exp(Tw)
- Bh = Pw exp(Tw)

即BB的中心点是fm上的位置加上预测出来的偏移，但是这个偏移sigmoid强制限制在0~1之间了，论文的目的应该是不让BB里fm这个位置太远，毕竟在fm上，这个位置的特征就是应该用来负责检测这个位置上的目标的；

然后BB的宽高就是预设的尺寸乘以缩放。


# Faster RCNN
Faster RCNN中RPN的Anchors原理解析：

得到60*40的feature map之后，对于map上的每一个像素，分别由

- 1x1x18(9 * 2)卷积生成每个像素的9个Anchor的分类信息，即该Anchor是否有目标

- 1x1x36(9 * 4)卷积生成每个像素的9个Anchot的偏移信息(Dx,Dy,Dw,Dh)，这个信息是用来加到固定的Anchor上的，也就是说，在配置中，我们固定了Anchor有3个大小，3中尺度，共3*3=9个Anchor，但是这种固定大小的肯定是和GroundTruth的大小不能完全匹配，而且负责生成这个Anchor的像素的位置和GroundTruth的中心位置也不能匹配，因此这第二个1x1卷积就是用来尽量的学到固定大小的Anchor和GT之间的大小的位置差距，从而使Proposal更加准确(当然没有Stage 2 的回归更准确，这里只是说相对的准确)。

其中，固定的基准Anchor是在训练一开始，初始化模型的时候就一次性生成好了，而不是每个像素进行1x1卷积时才生成的，这个固定的基准Anchor+RPN预测的偏移值，就得到了Proposal。

下边这段存疑,RPN生成的proposal应该就是anchor到GT Box之间的偏移,在求RPN_CLS_LOSS和RPN_PRED_LOSS时求的是anchor+proposal偏移之后和GT Box之间的差距.
> 在给每个像素生成固定Anchor之后，且在固定Anchor加上1x1conv的偏移之前，就需要求RPN_CLS_LOSS和RPN_PRED_LOSS，通过ground truth box与固定的基准anchor之间的大小和位置差异来进行学习，从而使RPN网络中的权重能够学习到预测box的能力。(这里没有求GT和"已经加了偏移的Anchor"之间的差距，从而进行学习的原因是，因为网络在每一轮训练或者预测开始时，只能获得到固定基准Anchor的大小和位置，无法获得上一时刻已经加了未经修正的偏移的、更准确的Anchor位置，因此实际上RPN要学的是基准Anchor和GT之间的差距，让这个差距越来越接近真实的差距，而不是学怎么预测出一个框使得这个框和GT特别接近，如果是这样的话Anchor就没有意义了)。

因此这里Anchor的作用就是，给RPN提供一个回归的基准，如果Anchor本身就很接近大部分目标的形状了，那么RPN就只需要很少的学习量，能够很好的提升学习性能；如果只用1个Anchor，或者不用Anchor，或者基础Anchor的大小和GT相差很大，那么RPN就需要很大的功夫去学习生成的Proposal和GT之间的偏差，甚至可能不收敛。


# Cascade RCNN
# FPN
# mmdetection
Unet系列用于semantic Segmentation
Mask RCNN系列用于instance Segmentation

对于mmdetection，当anchor_base_sizes为None时，使用anchor_strides充当anchor_base_sizes,然后用fpn每一层的map使用对应anchor_base_size，结合所有的anchor_scales和anchor_ratios生成框，进行前后景的分类。

anchor_ratio主要决定anchor的形状即宽高比；

anchor_base_sizes * anchor_scale生成所有的base_anchor；

然后base_anchor * anchor_strides生成各个feature_map上使用的anchor。

在进行anchor assign时，p2 ~ p6 在anchor_stride对应的5个值生成的所有anchor坐标是拼接在一起的，生成一个(p2+p3+p4+p5+p6,4)的tensor

mmdetection在config的backbone中可以直接配置dilation等参数
 
# HTC
> 这篇文章的Experiment里提到了很多COCO上涨分的技巧,可以学习一下.

![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/05/02/1588353289307-1588353289336.png)

- Cascade RCNN: 级联bbox head, 每一级预测的bbox输出作为下一级的输入,迭代refineb box.但是cascade没有考虑不同stage之间mask的信息传播,也没有考虑bbox分支和mask分支,强行在每一个stage 里面塞下了两个分支，但是这两个分支之间在训练过程中没有任何交互，它们是并行执行的;

- Interleaved execution: 每个stage先执行bbox回归,将回归之后得到的框交给mask分支,让mask分支能用到bbox的检测结果;

- Mask Information Flow: 这一步起到了很重要的作用，对一般 cascade 结构的设计和改进也具有借鉴意义。我们首先回顾原始 Cascade R-CNN 的结构，每个 stage 只有 box 分支。当前 stage 对下一 stage 产生影响的途径有两条：（1）$B_{i+1}$ 的输入特征是$B_{i}$ 预测出回归后的框通 RoI Align 获得的；（2） $B_{i+1}$ 的回归目标是依赖 $B_{i}$ 的框的预测的。这就是 box 分支的信息流，让下一个 stage 的特征和学习目标和当前 stage 有关。在 cascade 的结构中这种信息流是很重要的，让不同 stage 之间在逐渐调整而不是类似于一种 ensemble。
然而在 Cascade Mask R-CNN 中，不同 stage 之间的 mask 分支是没有任何直接的信息流的， $M_{i+1}$ 只和当前 $B_{i}$ 通过 RoI Align 有关联而与 $M_{i}$ 没有任何联系。多个 stage 的 mask 分支更像用不同分布的数据进行训练然后在测试的时候进行 ensemble，而没有起到 stage 间逐渐调整和增强的作用。为了解决这一问题，我们在相邻的 stage 的 mask 分支之间增加一条连接，提供 mask 分支的信息流，让 $M_{i+1}$ 能知道 $M_{i}$ 的特征。具体实现上如下图中红色部分所示，我们将 $M_{i}$ 的特征经过一个 1x1 的卷积做 feature embedding，然后输入到 $M_{i+1}$ ，这样 $M_{i+1}$ 既能得到 backbone 的特征，也能得到上一个 stage 的特征。
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/05/02/1588355070046-1588355070047.png)
- Semantic Feature Fusion: 这一步是我们尝试将语义分割引入到实例分割框架中，以获得更好的 spatial context。因为语义分割需要对全图进行精细的像素级的分类，所以它的特征是具有很强的空间位置信息，同时对前景和背景有很强的辨别能力。通过将这个分支的语义信息再融合到 box 和 mask 分支中，这两个分支的性能可以得到较大提升。
具体实现上,是在FPN后边加上一个简单的FCN(不是UNet那种Encoder-Decoder,就是一个straight-down的FCN),首先将 FPN 的 5 个 level 的特征图 resize 到相同大小并相加，然后经过一系列卷积，再分别预测出语义分割结果和语义分割特征。训练时的监督信息使用COCO Stuff全景分割数据集(这里在应用Semantic Feature Fusion的时候要考虑,自己的数据集里是否有这样的全景label).
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/05/02/1588355181545-1588355181546.png)



# PointNet
http://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf

# PointNet++
http://papers.nips.cc/paper/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space.pdf
MSG：取不同半径的点的特征拼接在一起
MRG：高层少数点特征pool+低层对应这些少数点的多数点特征pool拼接
无序主要影响的是没有局部信息

# SehllNet
http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_ShellNet_Efficient_Point_Cloud_Convolutional_Neural_Networks_Using_Concentric_Shells_ICCV_2019_paper.pdf

# Octree guided CNN with Spherical Kernels for 3D Point Clouds
https://arxiv.org/pdf/1903.00343.pdf

# PointCNN
邻居点feature和邻居点xyz提取的feature拼接，由于不同邻居点的顺序对生成不同的点集，学习一个排列矩阵，将邻居点顺序进行重新排列，从而实现点顺序的无关性质

# Deep Snake
Deep Snake Head：
1 输入是轮廓的N个点
2 经过多次circle conv，生成轮廓feature
3 进行1次1x1 conv，再max pooling，生成整个轮廓的全局feature(这里为什么用max pooling？汇集全局信息应该用avg pool啊)
4 再进行多次1x1 conv，得到最终的每个轮廓点的offset
用轮廓作为分割边界
1 首先生成detection的bb
2 选择bb四个中点，生成菱形
3 过一次Deep snake 生成物体的四个角点的offset(为了丰富边缘特征，实际上输入的40个点对应的feature，每个边被插值成了10个点，但是最后loss只负责监督输出的角点到bb的四角的offset)
4 分别以四个角点为中点，扩展出四条水平/垂直线(这四条线实际上就位于bb的四条边上)，其长度是bb的1/4；然后再把这四条线的两边连起来，最终构成一个八边形。这个八边形就比原来的矩形bb更好的框住了物体的轮廓，就把这个八边形作为snake的初始轮廓。
5 将这个初始轮廓采样成N个点，再送进Deep Snake，得到最终轮廓的N个offset(N一般是128)
6 重复i次步骤5(生成offset后，对原来的轮廓进行deform，然后将新轮廓采样之后再送入deep snake)，因为有的点离GT过远，需要反复进行偏移才能到GT附近
7 对于被遮挡物割裂的物体，使用multi component detector，先对Feature map做roi align，然后用一个多组件detector来检测目标，分别对每个组件做snake，最后再将轮廓合并

# Adder Net
把原来求互相关的conv转化为求L1-Norm(差的绝对值)

在bp过程中，原本的导数是sgn(X-F)，但是为了训练稳定，改为X-F，更精确；但是这个值可能太大(大于原本的+-1之间)，因此对与往前一层传的导数还要进行clip到+-1之间，使优化更稳定。

由于Adder conv的输出方差更大(见原论文)，需要借助BN来处理其输出；但是由BN的反传公式，方差越大，在BN层往前传的梯度越小，学不动了，因此需要适当增大学习率；但是通过实验发现，不同层的梯度大小分布也不同，因此不能对所有层的学习率都增大相同的倍数，而是使用了a~l~这个可调整系数，它与对应参数梯度的l2-norm成反比，也就是梯度越小，学习率相对更高，这样保证网络的不同层参数在优化时的实际更新的梯度是接近的。

# YOLO v4
## Bag of freebies
### 数据增强
随机裁剪旋转；
Random erase，CutOut；
DropOut；
MixUp，CutMix；

### 数据分布
hard example mining;
focal loss;
label smoothing;

### BBox regression
普通的BBox regression(回归角点，回归到anchor的offset)；
IOU Loss，GIOU Loss，DIOU Loss，CIOU Loss；

##  Bag of specials
### 增加感受野
SPP，ASPP，RFB；

### Attention
SEBlock,SAM;

### Feature integration
FPN,SFAM,ASFF,BiFPN;

### Activation
LReLU,PReLU,ReLU6,Swish,Mish;

### Post processing
Soft NMS；

## Backbone
需要感受野大，并且网络参数大，且输入图像够大
CSPDarknet53 + SPP + PANet + YOLOv3 Head

## 数据增强
Mosaic：就是大小为4的CutMix，找四张图像分别随机裁剪一小块下来，然后拼接在一起。这样可以让网络在被feed进一张图像时，接触到更多种类的contex的信息；另外就是BN在计算分布的时候用到了4张图像的统计信息，就不需要更大的batch size。

SAT：Self-Adversarial Training，循环执行两轮，第一轮固定网络参数训练扰动，目标是使检测结果达到最差；第二轮固定扰动正常训练网络参数，目的是在有扰动的情况下让检测结果达到最好
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/04/24/1587724175754-1587724175834.png)

CmBN：modified CBN，将1个batch划分为4个mini batch，对每一个mini Batch，分别accumulate W，再accumulate 这个mini batch对应的BN(传统的BN是accumulate这个大的batch中所有已计算的mini batchBN)，然后normalize BN，但是不更新W和BN的参数，直到最后一个mini batch算完之后再更新参数。

modified SAM：Spatial Attention Module
modified PAN：把add换成了concat


# Grid-GCN for Fast and Scalable Point Cloud Learning
Liu et al. [26] show that the data structuring cost in three popular point-based models [22, 45, 40] is up to 88% of the overall computational cost. (with Code)

提出了Grid-GCN，Coverage-Aware Grid Query (CAGQ).
1. 首先voxelize；
2. $O_v$是所有非空voxel，从中采样M个中心voxel $O_c$；
3. 因为已经构建了point-voxel的对应关系，就可以直接从映射表中查询某个中心voxel的邻居voxel（称之为context points）；
4. 从context points中随机采样K个点，计算这K个点的重心，作为这个点分组的group center；
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/04/29/1588099942606-1588099942631.png)

5. 这里有两个问题，第一个是如何选择中心voxel $O_c$，文章给出两种方法：
	1. RVS，纯随机采样
	2. CAS，Coverage-Aware采样，保证选择的点能最好的覆盖到整个空间，能覆盖到点最多的区域。具体做法是，先使用RVS随机采样M个点作为incumbent $V_I$；然后对于所有没被选择的非空voxel，依次作为challenger $V_c$，通过以下的式子(1~3)计算用$V_c$代替$V_I$所获得的覆盖率增益，如果这样做了之后能提升覆盖率，那就执行代替操作。
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/04/29/1588093912391-1588093912392.png)
- 在这个式子中，$C_V$代表点V的邻居voxel是incumbent的数量，$\lambda$代表邻居voxel的数量。
$H_{rmv}$代表删除incumbent后减少的覆盖率，其含义就是，对于$V_I$的某个邻居V，如果V的邻居中除了$V_I$之外还有其他的incumbent，那么$C_V-1$的值一定大于0，则$\delta$就是0，这说明删除了$V_I$之后，他的附近还会有别的点来代表他，因此覆盖率不会减少；反之，如果$V_I$的邻居的邻居都没有incumbent，这说明$V_I$附近都没有代表点，那么删除他之后这个区域的覆盖率就会受到影响而降低。
- 对于$H_{add}$，只看第一项，和$H_{rmv}$是一个概念，如果$V_C$邻居的邻居没有任何incumbent，说明添加这个点会增加覆盖率，反之也成立；而第二项是一个正则项是防止过度覆盖的，就是防止对$V_C$的选择过于宽松，导致最后过多的原incumbent被替代，结果采样的点过多。
6. 如何采样这K个点，同样有2种方法：
	1. cube query，类似于ball query，但是在点云密度不均匀的时候可以覆盖更大的空间。
	2. KNN。和全局KNN不同，这个KNN只需要搜索context voxel就可以。
7.  Grid Context Aggregation：每个点的特征由以下组成：
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/04/29/1588100887511-1588100887513.png)

	1. Coverage Weight，就是group center所连接的node的数量(这里有个问题，前边说了pick K node，难道这个weight不总是k吗？)；
	2. Grid Context Pooling，就是说不把中心点看做一个实际的点云中的点，没有feature（实际表现应该为，不存储这个中心点的feature到query tree中），而是在计算下一层的特征时，对group center的所有node的Feature做pooling得到一个feature向量；
	3. Edge attention,像pointnet++这类的网络没有考虑edge attention，如果pointnet2的group也看做一个图的话，那么就是中心点的所有edge连接的邻居的feature经过mlp之后，直接得到映射之后的feature，每个邻居(或者说edge)的权重是相等的；而edge attention则是给每一条edge再单独学出来一个权重，然后乘到mlp之后的feature上，从而体现出在图中不同邻居的重要性的不同。(这里有点像SEBlock，只不过SEBlock是在通道上的权重，而edge attention是在邻居节点这个维度上的attention)；
	4. Aggregation，最终对于点i，它的特征学习对应的输入是它的feature vector；它的edge attention对应的输入是：group center的坐标，点i的坐标，点i的coverage Weight，点i的feature vector(文章中称之为semantic)，以及Grid Context Pooling(即pooling之后得到的grouping center的feature)。在学习到下一层的feature和edge attention之后，将feature vectore和edge attention相乘，得到最终的feature。

 
# Federated Learning

## 优势:
- 网络带宽优势,只需要传输网络参数,不需要传输raw data
- 隐私性保护
- 低延迟,本地模型可以直接在本地做inference

##局限性:
- 网络带宽仍然是一个限制
- 边缘计算网络中各个设备的异质性
- model 参数中仍然有可能包含数据信息,因此隐私性仍然难以得到保护

FL的经典算法是FedAvg,就是端设备将学习完的参数传送到server之后,server把所有的参数按照子集大小加权平均下来,得到全局的最终权重

## FL framework
FL协议包括以下几个阶段:Selection,Configuration,Reporting.
Fl框架
- TensorFlow Federated (TFF)
- PySyft
- LEAF
- FATE

# FlowNet3D
# method
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/05/12/1589292505804-1589292505859.png)
pointcloud的光流计算难点:假设场景为刚体,并且需要找到对应点(这里是不是可以考虑few-shot learning)

1. 首先用pointnet++来提取点云特征
2. 在求光流时,首先对于前一帧的某个点的neighbor区域,找到后一帧也落在其中的点(有很多个)
3. 然后对这些点做一次卷积+max pooling,vote出和前一帧这个点对应的点(具体怎么vote这个是学习出来的)
4. 注意,另一求对应点的方法是,对于某个点,直接求其feature vector和目标帧neighbor中点的feature vector的距离,找到距离最近的那个点,但是作者证明这种方法没有3好.
5. flow插值:没用3d插值,而是对unknown set中的每个目标点,用knn或者ball query在原来的点少的known set中找到邻居点,然后过pointnet的conv,得到目标点的feature.作者通过实验说明这种上采样比原来的3d interp效果好.
6. Loss分两部分,一部分是SmoothL1 loss;另一部分是半监督的loss,即将目标帧的点云用预测的flow变换回源帧,然后加约束,让变换生成的源帧和真正的源帧相近.
7. 测试的时候使用了TTA,对点云进行多次重新sample,并重新inference,取多次平均

# dataset
FlyingThings3D, KITTI

# Depth Completion via Deep Basis Fitting
## 用lsf layer代替了FCN中的1x1卷积，来生成稠密的深度图；
思路是这样的：
在1x1卷积中，卷积核的参数是在训练过程中由监督信号(稠密深度)指导反向传播生成的；
而在lsf layer中，这个参数是直接由最小二乘回归得到的，回归的输入是由fcn提取的Feature map $B$，而这里回归的输出要注意，是稀疏深度图$S$(不是稠密深度图)，在计算回归的过程中，只选择$S$中的有效深度像素参与计算，这样即使在无监督(没有稠密深度)的情况下，也能够获取到足够多的feature map pixel - sparse depth pixel对，从而也能计算出最后一层的参数。
回归的目标函数：
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/07/1591467748535-1591467748557.png)

## 应用了M-estimator来实现Robustified Nonlinear Fitting

##使用了多尺度的深度图生成器
最终生成的深度图是将不同层级的decoder产生的B经过lsf之后得到的深度图相加在一起得到的。这个类似小波变换，不同层级的深度图的频率是不同的，加在一起合成了有丰富细节(频率分布的)深度图。


# Depth Completion from Sparse LiDAR Data with Depth-Normal Constraints
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/20/1592623863846-1592623863877.png)
1. 首先prediction net，预测出：
	1. coarse depth(loss用L2 loss)；
	2. Normal(normal的GT是通过depth的GT计算出来的，loss用的是负余弦距离，因为normal是一个向量，这里考虑为什么不用L2 loss？)；
	3. Confidence Map，代表输入的稀疏深度中每个点的可信度，可信度低的点更可能是噪声。由于这没有标注的GT，文章使用depth GT中的像素与sparse输入的差的负幂来计算这个值的GT，越大代表sparse中的点深度和depth GT越接近，也就越可信。在训练过程中confidence分支用来预测这个值，loss使用L2；

2.  plane-origin distance space. plane-origin distance P实际上就是某个点X的坐标向量和normal的内积，这个值对于所有在切平面的点都应该是相等的(x是切面里的点，X是数据点，显然切面里的点的法线都是同一个法线，积也是同一个值)。

![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/08/1591547372076-1591547372077.png)
同时，根据相机的成像模型，又能得到像平面坐标x到3维坐标X的转换公式，其中D(x)是深度，C是相机内参：

![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/08/1591547386991-1591547386992.png)

将上边两个式子联立起来，就能得到P和深度的关系，这样只要能预测出P就能得到深度D(x).
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/08/1591547396147-1591547396148.png)
这里用到这一空间的目的是利用法线的信息。

![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/20/1592623907630-1592623907631.png)

3.  diffusion. 这一过程是将coarse P中的每一个像素的信息向其周围的邻居像素传播，起到类似平滑深度和深度插值的作用。这一过程使用的是各向异性传播，即在使用近邻像素n的深度来计算当前像素x的深度时，要考虑n和x的相似度，越相似元素在计算的时候加的权重越大。

![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/08/1591550561118-1591550561138.png)
那么如何度量这个相似度呢，在经典算法里，由于除了稀疏深度之外还提供了可见光图像，所以就直接用两个像素对应的图像rgb值的差距来作为相似度。而在cnn里由于可以提取高维度特征，就使用feature map(原文中称之为Guidance feature)中像素对应的feature vector的距离来作为这个度量。注意在本文里，要求的而不是深度，而是平面距离P，在2中提到，只有在同一平面上的点才有相等的P值，因此本文在diffusion的过程中关注的相似度是两个像素对应的3D点是不是在同一个平面上，由于Guidance feature也包含了几何特征信息，因此G上相似的两个点就更有可能有相似的几何特性，点也就更有可能在同一个平面上，因此这篇文章也使用feature vector的距离来作为plane-origin distance space的相似性度量。

4.  迭代优化P. 迭代过程如下：
	1. 首先计算sparse P输入和coarse P输入融合之后的P，这个公式的意思是分别在spase和coarse中选择可用的像素点对应的P值(比如sparse中的空隙，就用coarse中的值来填充)，并加上Confidence权重 ![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/08/1591550944905-1591550944907.png)
	2. 对融合之后的P执行diffusion；
	3. 将diffusion之后的P作为新的coarse，重复步骤1，直到迭代完指定的次数。
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/20/1592623944320-1592623944321.png)

5. 最后用P和法线将深度还原出来。

×× 这篇文章之所以work，是基于这样一个假定：3D物体在局部结构上是由连续的平面构成的，这样其plane-origin distance在局部也就是连续的，作者认为这个假定要比之前算法的“2D平面上深度是局部连续的”要好，因此plane-origin distance map更加符合迭代difuse来refine的特性，那么通过迭代refine P，再还原回深度，就会得到更好的深度补全结果。同时增加了normal这一个分支，也就显性的增加了法线约束这个信息流。

但是这里有问题，在difuse的过程中，计算相似度用的是feature vector的距离，作者说这个距离近的点更有可能在同一个平面上，但是没有验证这一点。

另外normal map和P也都是预测出来的，这和直接预测深度区别很大吗？

# Voxel Net (With Code)
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/17/1592363484540-1592363484565.png)

![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/17/1592365803182-1592365803184.png)

1. 划分Voxel;
2. 把点Group到voxel的代表点上;
3. 每个groupd对应的vox里进行random sample;
4. 经过vfe layer，这里和pointnet2里的基本一样，就是把每个代表点的邻域(格子里)点的feature过一遍fc，得到新的feature，然后再经过element-wise pool的feature拼接在一起，构成最终的新feature，再过一遍fc，然后再将vox里的 所有点进行avgpool，构成最终的vox feature;
5. 对feature vox进行3d卷积，最后一层在channel这个通道上reshape成2d feature map，然后经过rpn，得到proposal，对应最后的result( rpn输出的map是H/2  x W/2 x 14的，其中14代表2 x (x,y,z,l,w,h,theta) )；
6. 在实现上，为了处理sparse的点云结构，使用了和pointnet2类似的点云邻居表，其中voxel索引是pointnet2的代表点。
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/17/1592365755164-1592365755168.png)


# SegGCN: Efficient 3D Point Cloud Segmentation with Fuzzy Spherical Kernel (with Code)
主要是提出了

# Single-Stage Semantic Segmentation from Image Labels
弱监督学习方法，训练时仅给定classification label，可以生成segmentation；
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/Screenshot%20from%202020-06-19%2019-15-01.png)
1. 使用类似FCN的backbone；
2. FCN生成的feature map(K x H x W)经过1x1 conv产生Score mask(C x H x W)，这里加入的监督条件是将Score Mask做nGWP(空间上进行pooling)，形成长度为C的feature vector，然后和弱监督label(即分类label)做L1 loss；
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/Screenshot%20from%202020-06-19%2019-15-10.png)
3. 对Score Mask进行softmax，得到伪分割label；
4. 对伪label进行iterative的refine，填充分割label的孔洞 ；
5. 使用Stochastic Gate（类似drop out），防止对伪label过拟合(因为伪label是无监督生成的，过分依赖这个会造成对质量差的label过拟合)
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/Screenshot%20from%202020-06-19%2019-15-20.png)

# KP Conv

# Real-Time 3D Model Tracking in Color and Depth on a Single CPU Core
使用的就是以物体中心为原点的深度表征；使用贝叶斯手段来判断像素/深度是否属于前景/背景

# Joint Spatial-Temporal Optimization for Stereo 3D Object Tracking
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/24/1592964002336-1592964002338.png)
1. 首先提取当前frame图像的feature，然后和前一帧的feature一起送到paired RPN中；
2. 在paired RPN中，只有和两帧GT Box的并集的IOU>0.7的proposal才被认为是foreground，这样保证了正类anchor一定会cover到两帧的box，这样这个anchor既可以回归到当前帧的box，又可以回归到前一帧的box；
3. 分别对cur和prev feature map上的ROI做ROI Align，再拼接在一起；
4. RPN生成一对box(b0,b1)，分别代表当前时刻t对应的前一帧I0和当前帧I1的目标，由于2中保证anchor一定cover两帧的box，b0和b1一定关联到同一目标；而对于t+1时刻的两帧I1和I2,I1和I0中的目标是通过逐个对比IOU来关联的；I2和I1的关联方式又和I0I1相同。
![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/06/24/1592964533849-1592964533850.png)

5. 对于3D box，将ROI Align之后的featrue进过FC之后，生成3D中心点(训练的时候学习的是ROI中心到3D box GT中心的距离，****这里有个严重的问题，有了ROI中心坐标之后不是可以直接用Velo-to-CAM投影矩阵就可以得到3D的点了吗)；3Dbox的尺寸是在coarse尺寸先验$f \frac{H_{3d}}{H_{2d}}$之上回归得到的；航向角直接是回归出来的；
6. 使用FCN获取前景object的segmentation mask；将object对应的depth转换到以object中心为原点的局部坐标系；并且预测Object图像每一个像素对应的3D局部坐标(相对于object中心)，文章说这样的表征保证同一个object的同一个part是有相同特征的；
7. 时空约束优化：
	1. 空间约束：最小化双目图像之间的重投影误差，这是用来监督稠密深度信息的；
	2. 时间约束：当前帧的前景投影到3d空间，减去当前帧的中心坐标(得到像素的相对坐标)，然后加上上一帧的中心坐标，再投影回上一帧的图像空间，这样就得到了当前帧前景在上一阵中的重投影，时间约束的目的是最小化这个重投影误差；
	3. 前景物体位姿约束：1 2都是对前景物体的相对位置进行约束，但是对于单独的一帧，为了保证检测出的目标位置是准确的，这里还要对前景物体的位姿进行约束（就是普通的Oriented 3D bbox loss）;
	4. Per-Frame Marginalization：这一步是为了更有效的利用前边更多帧的历史信息(因为1~3都是用的相邻两帧的信息)。这里实际上使用了一个8x8的参数矩阵H(8=4+4，相邻两帧$x_i$,$x_{i-1}$的[x,y,z,theta]向量拼接在一起)；对于第12帧，H是用高斯牛顿法求解约束为7.1,7.2,7.3的loss时所得到的hessian矩阵；而对于之后的所有帧，在求解目标位姿x时除了要满足7.1,7.2,7.3的约束，还要满足$||Hx_{i-1}-b||$最小，求解出这样的$x_i$之后，再用和求解第1 2帧相同的方法去更新H。这样在顺序处理所有帧的过程中，H就保留了前面若干帧的历史信息。
7. 在实验中，这个方法先在object 3D上预训练过。因为object中有一部分图像没在tracking中，用多出来这部分预训练前边的RPN和FCN，相当于扩充额外的数据集。

# ILCC
1. 先把LiDAR按照每条线的intensity反差分割成一段一段的线段集S
2. 对S中的线段进行聚类，具体做法是：
	1. 从所有剩余线段中取出第一段作为种子，放入当前cluster中；
	2. 遍历点云中所有线段，按照 (1)两线段中心距离以及(2)两线段所有点做PCA之后的协方差 作为距离，比较所有线段到当前cluster中线段的距离，将距离小于阈值的线段划分到当前cluster中；
	3. 重复1,2直到所有的线段都被划分到对应的cluster中了（有的cluster可能只包含一条线段，因为所有线段离他距离都远）；
3. 将marker从点云找出来，具体方法是先用PCA的方差判断点是不是在一个平面内，然后通过几何特性判断是不是长方体
4. 用混合高斯模型找到marker对应点云的最高和最低intensity分布的峰值对应的intensity，作为intensity二值化的pivot
5. 将marker对应的点云所在的平面拟合出来，然后把分布杂乱的点云全部投影到这个平面上；构建一个仿真的chessboard模型；将实际点刚体变换到xoy平面上
6. 使用optimizer求解旋转和偏移参数，使得在同一平面的真实点云和棋盘能够对齐。to optimizie的loss是：
	1. 如果真实点在chessboard模型的内部，判断真实点对应坐标的反射率是不是和模型一致(即真实点p在此处对应的棋盘格应该为黑色，则判断p的反射率是不是低于阈值TH，反之判断是不是高于TH)，如果不一致，则cost+=真实点到所有棋盘格的x偏移和y偏移最小值

# Feature-metric Registration: A Fast Semi-supervised Approach for Robust Point Cloud Registration without Correspondences

# Region Growing Segmentation

1. 随机生成1个seed，将seed放入队列
2. 以该seed为起始点进行BFS搜索，具体做法是：
	1. 从队首取一个点f，遍历其neighbor中的所有点p
	2. 如果p有label了，跳过；
	3. 检查起始点(注意是整个segment的起始点，不是点f)和s的曲率差、平滑度差等；
	4. 如果曲率和平滑度差距在允许范围内的，则p与起始点划分到同一个segment中，否则不在同一个segment中；如果p属于segment，则将p同时加入到BFS队列中；
	5. 重复1~5，直到BFS结束，获取到当前起始点对应连通区域的segment；
3. 更新已经被segment的点的数量和总segment数；
4. 从未被分割过的点里随机找一个点，作为新的起始点，返回步骤2，重复上述过程，直到所有的点被遍历到；

# RangeNet++: Fast and Accurate LiDAR Semantic Segmentation
1. 前边的部分不多说了，就是把点云投影到range image上；
2. 然后用FCN做分割；
3. 关键的部分是后处理，因为如果range image的分辨率过小，那么由于分割的粒度比较粗糙，必然有mask的边缘覆盖到物体的轮廓外边。如果这是图像还好办，但是如果是rangeimage，再重新投影回点云就会导致与当前这个物体非常远的，位于其后方远处的点也被赋予为这个物体的类别，因此文章要对分割结果进行后处理，具体过程如下：
	1. 用img2col构建[S^2^, hw]大小的col image, 其中列数hw就是图像所有像素，行数SxS代表在range image种每个像素邻近的SxS区域的像素。用这一结构能快速的找到某个点的近邻点；
	2. 当range image分辨率小，3D点多的时候，必然有点无法投影到range上，因此hw是小于N(点云数量)的，为了让这部分点的在分割结果中不丢失，文中方法实际上是保留了所有的点投影之后的uv pair的，因此在构建完col image之后，要把没投影上的点云补回col image中，变成[S^2^, N]的col image；
	3. 计算col image中每个点和其SxS个近邻点的range差；
	4. 使用反向高斯核对每个col进行反平滑(类似锐化，让远的更远，近的更近，这样不同距离的点就会分的更开)；
	5. 在col image中找到每个点的K个近邻点。在找的时候同时要考虑range距离阈值，如果这K个近邻点中有某些点和当前点的range差距太大，这说明在3D空间中这几个点肯定不是当前点的近邻，需要把这些range大于阈值的点再滤除掉；
	6. 对于每一个目标点，看其在5中找的近邻点中，那个类别的点最多，那么就把目标点的类别也设置成这个类别。
	7. 为什么这样work：因为在5过程中，已经把每个点的非3D近邻点都过滤掉了，那么在6中肯定是在目标点在空间上最近的点集中查找哪个类别的点最多。这样，对于之前因为分割粗糙，而造成的“前景物体和背景边界点分类错误”的现象（考虑路灯和其背景分割错误的例子），背景中的边界点由于距离前景物体的实际距离较远，这些边界点的近邻就基本不会包括物体中的点，这样在后处理的时候其实际类别就会被传播为与它们更近的背景点的类别，从而缓解上述的误分类问题。
	8. 存在的问题：如果误分类点的边界比较宽，那么即使经过了上述1~5过程，在边界的中心还是会有一些点仍然被传播为前景物体的类别(因为这些点的近邻都还是被误分为前景的点)。

# Learning to Segment 3D Point Clouds in 2D Image Space
这篇文章主要是用图布局算法，把3D点云紧凑的投影到2D图像上，保证在3D中的局部聚集区域，投影之后在2D空间上也是局部聚集的，这样就能够从CNN提特征了。

1. 首先对输入点云进行balanced kmeans聚类；
2. 然后分别对聚类中心，和每个类别中的点进行德劳内三角化，这样就得到了两个层次的图；
3. 使用图布局算法，先把聚类中心的图进行图布局出来，画到2D空间；再对每一个类别中的点三角进行图布局，画到2D空间，类别内图布局构成的2D图像就填充到中心2D图像对应的位置，此处可见论文中的图。
4. 然后用改进了的UNet提特征并分割，最后把2D图的分割结果还原到原始点云中。

# LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World
用真实LiDAR重建出的场景和真实LiDAR扫出来的物体构成仿真环境。
1. 对一个短序列点云做3D BB 标注，然后叠加每一个BB对应的物体点云到同一个物体，稠密表征；
2. 由于LiDAR扫出来的经常是半个车身，文章基于物体的对称性，直接把缺的那部分用完整的一半做镜像生成，然后再用intensity-based icp进行refine，这样讲数据集里的所有数据处理之后，得到几万个物体的Model, model使用suferl来存储；
3. 仿真LiDAR scan是用ray cast生成；
4. 为了仿真ray drop，用了一个网络来学drop prob mask，对于每一个laser点，用mask上对应的prob进行采样，决定是否对这个点进行drop；网络的输入是场景+2中重建的物体构成的model scene，投影之后的range image，GT是用model scene与真实的LiDAR scan相减得到的drop mask(相减的差就是真是场景中丢掉的点)，网络学的是每一个点是否进行drop的prob。（这样在测试的时候，对于任意物体和方向组合的场景，网络应该有能力推理出哪些点要被drop掉）。

# Deep Hough Voting for 3D Object Detection in Point Clouds

# Loam horizon livox代码
没有后端；没有回环检测；
与环状雷达不同， horizon的扫描线即使在平坦的表面也会出现曲率变化，这可能会对角点的提取造成影响；

1. 预处理，运动畸变消除。每一帧点云对应一个imu序列，imu进行过积分之后对点云中的每一个点做运动补偿；注意在这个loam算法里，imu仅用于对原始点云进行运动畸变消除，而没有用于建图过程；
2. 特征提取，将Horizon看成一个六线雷达，在每条线上提取两类特征：角点和平面点
	1. 首先计算每条线上所有点的曲率；
	2. 计算每条线上所有点与邻居点的梯度，左右地图都过大的认为是噪点，被滤除掉；
	3. 将每条线划分成长度为50左右的线段，对线段中所有点的曲率进行排序；
	4. 如果线段中的最大曲率大于线段剩余点的曲率和，则认为这个大曲率点是scan的拐点，将其移除；
	5. 在每一段中，在曲率大于阈值的点中，找到曲率最大的K个点，作为sharp特征点，剩下的作为less sharp特征点；
	6. 同时将这些曲率大于阈值的点的邻居点设置为不可选取，这样可以保证特征的稀疏性；
	7. 用同样的方法，在曲率小于阈值的点中，找曲率最小的为flat特征，次小的为less flat特征；
	8. 把sharp合并到less sharp里，flat合并到less flat里。在后边的匹配过程中，当前帧(source)使用sharp/flat特征(较少)，在过去帧(dest)的less sharp/flat特征里(较多)进行匹配；
3. laser里程计。这部分是使用帧间特征匹配来求解两帧点云的相对位姿。(last表示上一帧)
	1. 对于新到的一帧点云，按2的方法提取特征；
	2. 用上一次求解的相对位姿将当前帧和last对齐(对齐的肯定不准，但是考虑连续两帧运动不大，可以做一个近似的初始值)；
	2. 对于角点特征，使用点-线ICP，具体方法是，对于当前帧的每一个点，在last less sharp中找他的邻居点，然后通过pca判断邻居的形状，只有邻居构成一条线(协方差的最大特征值远大于其他两个)才用这个特征点参与计算；
	3. 角点对loss的贡献就是角点到邻域线的距离；
	4. 对于平面特征，使用点-面ICP，具体方法是，对于当前帧的每一个点，在last less flat中找他的邻居点，然后拟合邻居点构成的平面；
	5. 平面点对loss的贡献就是点到邻域平面的距离；
	6. 对所有特征点的loss优化之后，就得到两帧之间的相对位姿；将每一帧的相对位姿累积起来，就得到一个粗略估计的，有累计误差的雷达里程计；
4. laser建图。这部分是对3估计出来的里程计进一步优化，使用当前帧-地图的匹配。
	1. 首先将之前已经建好的地图划分成多个cell；
	2. 用上一帧估计出来的位姿将新到的点云对齐到地图坐标系下；(对齐的肯定不准，但是考虑连续两帧运动不大，可以做一个近似的初始值)；
	3. 找到当前这帧点云在哪个cell中，将这个cell和邻居cell(比如10m范围内)的特征点和特征平面地图都取出来；
	4. 用3.2和3.3中同样的办法，通过点-线和点-面匹配来计算出当前帧在局部地图中的绝对位置(由于局部地图和全局地图在同一个坐标系，确定了局部地图就是确定了全局地图的位置)；
	5. 得到的位置就是当前帧点云最终估算出来的位置，用当前位置对3估计出来的里程计位置进行逆变换，就是map到odom的变换；
	6. 得到最终位置之后将当前这帧的特征点云和原始点云加到地图中，再对更新的地图做下采样。


# Livox 重定位代码
问题：特征提取是对mid40过拟合的；重定位没有用粒子滤波，很容易出现裹挟，且初始值错了就是错了；依赖里程计；

1. 特征提取，主要还是提取两类特征，并且将整个点云看成一条laser
	1. 第一类是平面特征，通过计算每个点的曲率，将曲率小于阈值的标记为平面点；
	2. 第二类是角点，分为两种：
		1. 第一种是两个平面相交形成的边缘点，将角点处夹角足够接近90度并且有足够邻域长度的认为是角点；
		2. 第二种是由于前景遮挡形成的边缘点，这里注意不要提取假边缘，即背景中由于前景遮挡形成的阴影边缘；
2. 定位。
	1. 首先载入已经建好的地图的原始点云、角点和平面特征点云，以及关键帧位置序列;保持雷达不动大概10s，获取积分多帧后的点云作为初始配准点云；
	2. 遍历关键帧序列，以每个帧的位姿作为icp的初始值，对当前点云和地图的角点特征进行icp，找到匹配度最强的那个关键帧作为定位的初始值；
	3. 将2中拿到的初始值再次作为icp初始值，进行迭代次数更多的icp，求解出更精确地初始位姿；
	4. 后边的过程和loam中的laserMapping一致，对于每一帧新到的点云，通过点-线icp和点-面icp求解这帧点云到在整个地图中的位姿(在loam中是当前与点云邻近的局部地图，在这个重定位里就是拿当前整幅地图做匹配)；

# A-LOAM 
和LOAM在算法上完全一致，只不过点云做icp的时候从手写的变成使用ceres-solver

# Loam Livox代码
注意，在实现的时候，对于MID100或者多个MID40雷达，都是对每个MID40单独提取信息，因为下边的特征提取方法是专门针对petal形状设计的，然后再将提取的特征融合在一起。这样很快就能泛化到多个mid40建图，只需要在融合的时候用外参拼接一下特征就行了；
1. 针对MID40/100的预处理
	1. 将inf点、距离lidar过近、反射率过低、太靠近petal边缘和中心的点标记为非特征点；
	2. 计算每个点的曲率，深度，极半径等信息，方便后边的计算；
	2. 将lidar投影到归一化的极平面上(和相机模型类似)；
	3. 根据当前点和其邻域的关系，判断其是远离极点还是靠近极点方向的(因为点都是顺序获取到的)
	4. 找到在两种方向交接处的点，作为分割点；
	5. 根据分割点，把petal划分成一条一条的scan，scan的id就是petal在这一段的切线角度；
	6. 这样就将petal形状的点云转换成类似多线雷达的表达形式；
2. 后边的过程和LOAM类似 

# Oriented Object Detection in Aerial Images with Box Boundary-Aware Vectors
这篇文章的基础是，传统的w,h,theta的方法，在theta有微小偏差的时候，网络只能得到一个很小的loss，而这个小偏差在测试的时候却对IOU影响很大；因此这篇文章想让theta有小偏差的时候loss也有较大的偏差；另外就是传统方法学到的theta是基于y轴的航向角，跟目标没什么关系，这篇文章想让网络学到航向角在目标局部坐标系下和目标自身的关系

将BB划分为HBB(非倾斜的BB)和RBB(有倾斜的BB)两类，这是因为，trbl是按照四个象限划分的，如果BB的角度接近90度或者接近0度，在坐标轴附近摇摆的时候，trbl会出现很大的抖动(比如在坐标轴右侧1度的时候还是[trbl]这样的排列，这时候只要向左倾斜2度，就会变成[lbrt]的排列，那么BB的descriptor向量的值就会有很大的值变动，但是实际上角度只改变了2度)；

因此，对于HBB，在forward的时候直接按四个角的普通的无orientation BB来计算四个角坐标；

而对于RBB，在forward的时候是按照trbl向量来计算四个角坐标；

怎么区分HBB和RBB：用一个2分类子网络分支；

注意，这里有个非常大的问题，这个文章在网络进行学习的时候来解决H/RBB的问题，而仅仅通过分类结果来做后处理，也就是说网络还是没有能力处理corner case；这块有很大的改进空间；最简单的思路就是在这种corner case的地方换回普通的w，h，theta这种的bounding box；

# Lego Loam代码解读
## image projection
1. 首先找到这一帧点云的首点和末点的航向角,以及一共转过多少角度,这部分在后边用imu做运动畸变消除的时候要用到,比如用来求当前某个角度的点相对于当前帧起始点的相对时间;
2. 将点云投影到图像平面,这里不使用透视投影,而是直接将点一个个平铺在像平面,其中深度是点到LiDAR中心距离;
3. 检测哪些点是地面点,方法是对于每个点,求其与上边一条线上的相同竖直位置的点的相对俯仰角,如果小于10度,就认为是地面点;
4. 对深度投影进行区域增长分割, 具体方法是
	1. 以每个未分割的点作为种子进行BFS, 
	2. 在搜索的过程中只要 当前点至邻域点的连线 与 视线(雷达中心到当前点的方向，即轴向)夹角大于一个阈值, 就将邻域点扩张到当前的分割区域中;
	3. 原因是, 邻域的步长是确定的，这个夹角越大说明在3D空间中当前点和邻域越近;
	4. 最后判断一下当前分割出来的区域点数,以及区域跨越的lidar线数,如果区域过小,或者跨域的线太少,认为是outlier构成的区域,将其舍弃;
	5. 后边的特征点面提取就在剩下的有效segmentation里,这样可以消除噪声影响.
5. 将各类点云publish出去;

## feature association
1. 如果有imu的话,将每一帧imu存到环形队列里; 然后对于当前帧的每一个点, 计算出其相对于当前帧到达时间的相对时间, 然后找到imu队列里离这个时间最近的两帧, 通过插值来求得这个时间对应的LiDAR本身的角度和速度, 然后对当前点进行运动补偿; 
2. 同时用imu插值出来的速度更新对lidar运动速度的估计，这个速度用来计算下一帧初始位置的猜测量;
3. 下边特征提取和loam很相似:
	1. 在每条线上, 通过邻域点计算每个点的曲率;
	2. 把背景中由于遮挡而形成的假边缘点过滤掉,具体做法是先判断同一条线上相邻两个点的的深度差,如果深度差过大,这说明有一个点是前景点,另一个是背景点; 判断是前景还是背景的规则很简单,如果左侧深度大于右侧,这就说明左侧是由于遮挡引起的,就将左侧标记为不可被选取的边缘点,反之亦然;
	3. 在非遮挡假边缘/飞点的正常点集中, 提取四类特征，sharp，less sharp，flat，less flat;
	4. 注意, 提角点时不考虑地面点，提平面特征的时候只考虑地面点(这里是不是有问题，如果有大面积的墙或者天花板怎么处理); 
4. 用局部积分的imu来作为两帧之间相对位移的初始估计值;
5. 和LOAM一样, 用点线匹配(约束为点在线上)和点面匹配(约束为点在面上);
6. 但是这里要注意, 
	1. 在使用来自地面的面特征进行点面匹配时，只优化Tz, Troll, Tpitch, 即用地面信息计算出竖直方向的变动;
	2. 当算出竖直变动后，可以以此为初值输入到第二步的优化中，减少迭代次数，算出水平维度的变动;
	3. 在点线匹配时，只优化Tx, Ty, Tyaw, 即只优化水平方向的变动;
	4. 这样可以充分利用来自特征的不同特性;
7. 把每一帧的帧间匹配结果累积起来，构成里程计; 注意因为特征匹配优化是在imu的角度的作为初始解的基础上进行的，得到的相对位移是相对于imu积分的值，因此还要在这一步把imu的测量值减回去;
8. 发布原始仅通过累加的里程计;

## map optimization
map优化不是实时进行的, 而是有一定间隔以减少计算开销

1. 利用上一帧估计的绝对位姿+前后两帧里程计的相对变化 来作为绝对位姿的初始值;
2. 找到当前位姿邻近的所有关键帧位姿. 
	1. 如果开启回环, 就直接使用最新的若干帧特征来构建局部地图;
	2. 如果不开启回环, 则将全部关键帧位姿点放到kdtree里, 在kdtree里找到当前帧的邻近关键帧, 再将这些关键帧对应的点云拼接成局部地图;
3. 对当前帧的特征点云进行下采样;
4. 用帧间匹配相同的方法来做帧-局部地图匹配, 求解出当前帧在地图中的绝对位姿;
5. 如果使用imu, 则将估计出来的局对位姿和imu的积分值进行加权相加, imu加的是一个很小的权重;
6. 用地图优化之后的值来更新当前底盘位姿; 根据当前位置与上一个关键帧的距离是否足够远, 来确定当前帧是否为关键帧; 
7. 如果是关键帧, 将(上一关键帧的绝对位姿, 当前关键帧的绝对位姿)作为边加入到因子图中, 边的观测值是当前到上一帧的相对位移; 注意如果是最开始的第一个关键帧, 则放prior factor;
8. 用isam来增量优化因子图; 优化之后, 保存当前关键帧的优化后的位姿到历史关键帧列表中;
9. 如果在这个因子图优化之后(或者过程中)检测到回环成功, 那么就将存在因子图中的所有优化后的关键帧的位姿更新到历史关键帧列表.
10. 发布优化后的绝对位姿;

## loop clouse
回环是一个独立的线程, 循环做以下事情:
1. 检测是否存在可能的回环, 方法是在当前位姿半径为historyKeyframeSearchRadius的范围内搜索历史关键帧列表, 只有当前位姿附近有历史位姿, 且历史位姿和当前位姿的时间差较大时才认为可能存在回环;
2. 如果存在回环, 把满足条件的, 离当前帧最近历史位姿作为可能回环的位置, 把这一位置的前后historyKeyframeSearchNum个关键帧的所有特征点(角点和面点)拼接起来构成局部地图;
3. 直接使用icp, 配准当前帧点云和局部地图, 获取到当前和局部地图的相对位姿; 
4. 在因子图里添加一条边, 一端是当前关键帧位姿, 另一端是在历史关键帧位姿列表里存储的, 距离当前帧最近的一帧(就是在2中描述的那一帧);
5. 而边的测量值, 就是当前帧到历史最近帧的偏移(这个偏移是准确的,因为刚才已经通过配准将当前帧对齐到历史局部地图的坐标系了, 而历史最近帧就在历史局部地图里);
6. 加上回环之后再次用isam增量优化因子图;


## transformfusion;
这一步主要是将原始的累计odom转换到map坐标系下

# Attention is all you need
## self attention layer
1. 词向量矩阵X(句子长度*特征数量)分别经过三个FC, 变成K,Q,V;
2. 求矩阵乘积KQ',这里存的是每一个词特征向量和另一个词特征向量的乘积;
3. 对KQ'归一化之后并softmax之后,得到序列每一个词向量对当前词向量的响应值, 把所有响应值加在一起,就得到了当前词向量在经过SA之后的输出;
4. Query，Key，Value的概念取自于信息检索系统，举个简单的搜索的例子来说。当你在某电商平台搜索某件商品（年轻女士冬季穿的红色薄款羽绒服）时，你在搜索引擎上输入的内容便是Query，然后搜索引擎根据Query为你匹配Key（例如商品的种类，颜色，描述等），然后根据Query和Key的相似度得到匹配的内容（Value)。

## multi head attention
1. 同一个X输入多个SA中,得到多个SA输出;
2. 多个输出在列方向拼在一起;
3. 过一个FC,又变成和X一样shape的embedding;

## Encoder-Decoder Attention
在解码器中，Transformer block比编码器中多了个encoder-cecoder attention。在encoder-decoder attention中， Q来之与解码器的上一个输出， K和V则来自于与编码器的输出。其计算方式完全和图10的过程相同。

## 位置编码
Transformer模型并没有捕捉顺序序列的能力，也就是说无论句子的结构怎么打乱，Transformer都会得到类似的结果。换句话说，Transformer只是一个功能更强大的词袋模型而已。为了解决这个问题，论文中在编码词向量时引入了位置编码（Position Embedding）的特征。具体地说，位置编码会在词向量中加入了单词的位置信息，这样Transformer就能区分不同位置的单词了。

这里的位置编码其实就是X的特征在里加上词在句子中的绝对和相对位置, 详见https://github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9429b5c40a139/tensor2tensor/layers/common_attention.py

# liosam
1. 先对imu进行预积分
2. 不做scan2scan的匹配,而是做scan2map的匹配,这个map是当前位姿附近的若干个关键帧点云构成的.注意在初始化的时候, 由于不存在map,所有的输入帧都被认为是关键帧,并且这些初始帧间的相对位姿使用imu预积分来估计的;
3. 仅当估计出来的位姿方差大于GPS方差时才加入GPS因子;
4. 回环因子可以有效的消除GPS在高度方向上的误差;

imuIntegratorImu_维护的是当前的最新imu预积分出来的位姿,它会在两种时刻发生变化:
1. 每当有一个新的imu到达的时候,imuIntegratorImu_会在之前预积分位姿基础之上,对新数据进行积分,得到高频率的imu位姿估计;
2. 每当通过map odom和之前的预积分联合完成一次因子图优化的时候,imuIntegratorImu_会利用优化出来的最新bias,对imu队列所有最新数据进行一次预积分,得到实时的估计量,尽可能保证imuIntegratorImu_在此时刻的估计量所使用的bias是最新的


# SLOAM
在茂密树林环境下,针对树木语义特征的SLAM

这里语义信息是这么用的: 

1. 这里对cylinder的参数化请参考
Faithful Least-Squares Fitting of Spheres, Cylinders, Cones and Tori for Reliable Segmentation
2. 对于Tree-feature, 先用初始guess将上一帧投影到当前帧的位置,然后对于每个tree-feature point, 在下一帧中找最近(或者一定半径范围内)的tree landmark. 在数据关联的时候,求解的是上一帧的tree-feature point到下一帧特征cylinder的一个点-圆柱面的匹配;
3. 对于ground-feature,每次从当前帧中找到下一帧地面点的近邻点,然后用PCA计算近邻点集合的平面参数,而不是时时维护一个地面参数(这不就和loam的点面匹配一样嘛?而且他是怎么分割出地面的?)
4. 在建图过程中:
	1. 初始的map-tree-feature为空;
	2. 对于每一帧新的点云,对其语义分割之后得到新的tree feature,与map中的feature进行数据关联;
	3. 如果map中没有与其匹配的关联,就把新的feature标记为unassigned;
	4. 检查所有的unassigned,把可能属于同一个tree的聚集在一起,构成一颗新tree加入到map中;
	5. 如此重复建图
5. 树木分割是把点云投影到range image,然后用erfnet做分割
6. 地面分割,是先把树木去掉,然后把生喜爱的点划分多个格子,每个格子里取最低的点
7. 每一个树木的实例检测, 用的是维特比算法,就是经过语义分割之后的range image, 作者观察到所有雷达线上,同一棵树对应的点cluster正好能够构成一个trellis图, 只要从雷达的最高线往雷达的最低线查找,构成树的哪些点形成的最短路径,一定就是一棵树的instance(因为重力,树基本都是上下直着生长的). 其实就是类似区域增长法,只不过是查找直线.找到树的实例,用其点来拟合前边提到的圆柱参数.

# overlap net

delta-head：计算两个点云的重叠程度
具体做法是，对于双生网络的两个leg输出的HxWxC的feature map，先将其展开成HWxC的一维feature，然后对其中一个在横向复制HW倍，另一个在纵向复制HW倍(变成两个HWxHWxC的feature map)，然后对这两个进行逐元素求差异。这样的操作可以保证两个feature map的每一个元素都会与另一个的每一个元素做一次相关，从而具备了学习两个输入偏移的的所有信息。

correlation-head：求解两个点云之间的yaw角偏移：
对于双生网络的两个leg输出的feature map，将其中一个复制1倍之后，用另一个做卷积核去卷他，以这样的方式对二者做互相关，然后相关的结果是一个360维的向量，对应360个度，激活值最大的那个位置对应的向量就是二者的yaw偏移。

可以这么做的原因，是因为输入的是两个rangeimage，是在球面空间的投影，并且我们认为点云是水平的，不存在roll和pitch偏移，这样二者在range image(或者feature map) x方向的偏移就可以用来表示二者yaw角的偏移。

# Learning an Overlap-based Observation Model for 3D LiDAR Localization
这篇文章整体框架用的还是MCL，与之前工作不同的是，文章没有用深度学习来做场景识别， 而是把MCL中的观测模型换成了可以学习的；

文章的releated works可以多看一下，说的比较全面

1. 先把点云投影到range image；
2. 以一定分辨率把地图划分网格，每个网格产生virtual scan。实际存储的时候，直接存储virtual scan经过一路双生网络Encoder之后的到的embedding，这样可以极大的节省存储空间，并且在实时匹配的时候就不用再把virtual scan过一遍Encoder了；
3. 这里简单提一下MCL：
	1. 维护一堆粒子，每个粒子代表底盘的可能位置；
	2. 当底盘运动时，用运动模型来估计每个粒子下一时刻的状态，然后根据这些状态的观测量，和底盘实际获得的观测量之间的相似程度，来更新粒子的权重；
	3. 然后根据新的权重对粒子重采样，大概思想就是估计的更准的那些粒子有更大的概率被采样到(这样即保证留下了准了，又保证那些可能的更优秀解能被探索到)
	4. 3里所说的，就是下边这个公式描述的，积分那项是所有粒子都进行运动估计，观测那项是在当前的位置，观测到当前这一帧LiDAR输入，有多大可能性（其实就是overlapnet的输出。overlapnet学到的就是LiDAR scan随空间的概率分布，这里的空间是通过virtual scan来表示的，因为每个virtual都独立的代表了一个空间位置。所以P(z|x)就相当于overlapnet(LiDAR scan|virtaul scan)）；
 ![title](https://raw.githubusercontent.com/HViktorTsoi/gitnote-image/master/gitnote/2020/12/08/1607435978428-1607435978431.png)
	5. 作者说用overlap之后，在正确定位点附近的overlap值仍然很高，这样就可以不需要用过多的粒子去填充整个地图，少量的粒子就能保证至少有一部分是在全局定位附近的；另外由于划分网格了，如果多个粒子落在同一个网格，overlapnet只需要做一次inference；
	6. observation分两块，一个是delta，就是当前位置的map有多大可能性与输入scan匹配；另一个是yaw，就是（不管匹不匹配）当前scan和map的相对yaw angle。
	7. 作者从实验里验证了方法非常块，因为多个落在同一个格子里的粒子只需要过一次网络(overlap网络的设计使其有这个功能，参照第5步)


# Neural Topological SLAM for Visual Navigation
Graph update：
如果成功将新图像定位到某个node，就将当前所在的node链接一条边到新匹配的这个node上；如果没匹配成功就新建一个node，同样也连一条边从当前node当新建的这个node；

为了实现explor，在每个位置上，在每一个下一步可以探索的方向上添加ghost node，由于ghost node上没有实际的观测值(因为没到过)，就把当前node的全景图裁下来一块(和ghost对应方向的)，作为ghost node的观测。如果真正到过ghost了，就把这个ghost删除掉。直观上所有ghost构成了自主探索的frontier。

global policy：
首先还是用图定位，如果定位了某个node，那这个node就是最终的全局目标；如果没有定位，就要选择一个ghost node作为全局目标，选择的方式是使用Fs，看哪个方向最有可能到达最终目标；

选定了全局目标之后，就在图上用简单的dij方法找到一条拓扑最短路径，最短路径上第一个node就是subgoal；

如果全局目标就是当前node(到达目标)，那么再用Fr来估计当前位姿到目标位姿的偏差

local policy：
是一个pointgoal navigation policy，根据rgb/rgbd预测一个局部地图，然后在局部地图上规划一个最短路径，移动过去。

整个算法中需要学习部分的流程：
1. 给定一个当前图像，和goal图像
2. 首先用定位网络判断是不是同一个node，如果不是，就进行node间的prediction，输出各个方向的几何可行性和语义可行性；
3. 如果是同一个node，进行node内prediction，输出当前位置到最终目标位置的相对位姿

训练：
是在一个有室内环境的VR环境中Gibson训练的，这样就可以既得到观测量，又可以进行探索


# CMRNet: Camera to LiDAR-Map Registration
这篇做的是通过可见光相机，在LiDAR构建的点云地图上进行定位。作者说他们的工作是受RegNet启发？？？RegNet是一个做相机-雷达外参标定的网络。

方法的输入是
1. 预先构建好的点云地图
2. 单帧可见光图像
3. 一个粗糙的初始位姿估计

整个方法的流程和外参标定非常像：
1. 首先得有个相机位姿的初始估计。根据估计的位姿，将FOV内的地图点云投影到相机的成像平面(用实际使用的那个相机的内参)，得到一个depth image；在投影的时候，用了zbuffer test来保证前后深度正确；
2. 一个比较关键的问题是，由于map点云仍然比较稀疏，投影之后某些在前景的点云的缝隙里就会把背景的点云透显出来，这肯定会影响深度图的精度（在我们做上采样的时候这个问题不明显，因为是单帧点云；如果是地图就明显的多了，因为FOV外边会有很大范围的点云）；为了解决这个问题，
2. 这个时候depth image和输入的可见光图像是不对齐的，如果能把他们对齐了(就是外参标定问题)，那么可见光图像也就和点云地图对齐了，这就实现了定位；
3. 标定的是用CMRNet实现的，输入是不对齐的depth image和可见光，输出是相对位姿；

怎么解决拖影问题？
投影的时候只删除了