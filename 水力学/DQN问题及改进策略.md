1. 第一点是对连续动作的处理能力不足。DQN之类的方法一般都是只处理离散动作，无法处理连续动作。虽然有NAF DQN之类的变通方法，但是并不优雅。比如我们之前提到的经典的冰球世界(PuckWorld) 强化学习问题，具体的动态demo见这里。环境由一个正方形区域构成代表着冰球场地，场地内大的圆代表着运动员个体，小圆代表着目标冰球。在这个正方形环境中，小圆会每隔一定的时间随机改变在场地的位置，而代表个体的大圆的任务就是尽可能快的接近冰球目标。大圆可以操作的行为是在水平和竖直共四个方向上施加一个时间乘时长的力，借此来改变大圆的速度。假如此时这个力的大小和方向是可以灵活选择的，那么使用普通的DQN之类的算法就不好做了。因为此时策略是一个有具体值有方向的力，我们可以把这个力在水平和垂直方向分解。那么这个力就是两个连续的向量组成，这个策略使用离散的方式是不好表达的，但是用Policy Based强化学习方法却很容易建模。
2. 第二点是对受限状态下的问题处理能力不足。在使用特征来描述状态空间中的某一个状态时，有可能因为个体观测的限制或者建模的局限，导致真实环境下本来不同的两个状态却再我们建模后拥有相同的特征描述，进而很有可能导致我们的value Based方法无法得到最优解。此时使用Policy Based强化学习方法也很有效。
3. 第三点是无法解决随机策略问题。Value Based强化学习方法对应的最优策略通常是确定性策略，因为其是从众多行为价值中选择一个最大价值的行为，而有些问题的最优策略却是随机策略，这种情况下同样是无法通过基于价值的学习来求解的。这时也可以考虑使用Policy Based强化学习方法。

多Agent 第一个是充分利用计算资源 第二个是把优化目标分解 规约成多目标优化

dynamic result

分水 实际能看到吗? 
前馈

input 给出的流量 output计算给出的就是水位
input 给出的水位 output计算给出的就是流量
上下游 流量 上下游水位

不应该直接在水位超限的时候直接崩溃 应该给一个大的负分
不应该在多个时间片崩溃时直接给停止,而是应该给一个大的负分

- 使用A3C模型,无需经验回放,通过多个agent实现batch训练,每次采样选择个完整的episode
- Actor:采用策略梯度; Crtic:采用值梯度
- 分水时间,分水量
- 使用LSTMCell 而不是整个LSTM
- 训练的同时,有一个worker单独测试
- 成功结束时，给一个大的奖励；失败结束时，给一个大的惩罚
- 多个loss求和后backward vs 计算多个reward之后再计算loss 单个loss回传
- DQN版本 **不应该**把reward 和 action 也作为观测量
- 测试时 不应该采用$epis$贪婪策略选择action 而是应该直接用最高价值action或者策略进行选择
- endstate为1和0时应该保证持续调控reward是累加的

稳态 完全收敛参数
# =======================================
# 强化学习模型相关
# 线程数
WORKERS = 21
# 学习率
LR = 0.0009
# 衰减权重gamma
G = 0.98
# loss中entropy所占比例
ENTROPY_WEIGHT = 0.05
# gae参数T
T = 1.00
# 最大episode长度
NS = 133
M = 10000
# 优化器
SO = True
OPT = 'Adam'
# 随机种子
seed = 42
# 奖励表
reward_table = {
    'abs_0_5': 0.03,
    'abs_5_10': -0.01,
    'abs_10_15': -0.03,
    'abs_15_20': -0.06,
    'abs_20': -0.5,
}
失败:reward = [r - 5 for r in reward]
成功:reward = [r + 15 for r in reward]
每轮:reward = [r + 0.001 * self.eps_len for r in reward]

